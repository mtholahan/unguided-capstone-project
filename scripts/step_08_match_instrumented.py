"""
Step 08: Match TMDb (Instrumented, Refactored)
----------------------------------------------
Performs exact + fuzzy matching between normalized TMDb and MusicBrainz datasets.
Uploads results and metrics to Azure Blob (optional).
Refactored for BaseStep consistency and PowerShell-safe progress iteration.
"""

from base_step import BaseStep
import pandas as pd, requests, numpy as np, os, argparse, time
from rapidfuzz import fuzz, process
from config import TMDB_DIR, DEBUG_MODE, TMDB_API_KEY, AZURE_CONN_STR, BLOB_CONTAINER
from utils import normalize_for_matching_extended as normalize
from azure.storage.blob import BlobServiceClient
from pathlib import Path

FUZZ_THRESHOLD = 55
TOP_N = 5
FORCE_RENORM = False
USE_ALT_TITLES = True
YEAR_VARIANCE = 2  # moved from config


# ============================================================
# Helper: Azure-safe overwrite + optional upload
# ============================================================
class SafeWriterMixin:
    def safe_overwrite(self, df: pd.DataFrame, path: Path, upload_to_blob=False):
        """Write CSV atomically, then optionally push to Azure Blob."""
        tmp = Path(f"{path}.tmp")
        df.to_csv(tmp, index=False)
        os.replace(tmp, path)
        self.logger.info(f"ðŸ’¾ Wrote {len(df):,} rows â†’ {path.name}")

        if upload_to_blob:
            self.upload_to_blob(path)

    def upload_to_blob(self, local_path: Path):
        """Upload a local file to Azure Blob Storage."""
        if not AZURE_CONN_STR:
            self.logger.info("â˜ï¸ Azure upload skipped (no connection string set).")
            return
        try:
            blob_service = BlobServiceClient.from_connection_string(AZURE_CONN_STR)
            blob_client = blob_service.get_blob_client(
                container=BLOB_CONTAINER,
                blob=f"outputs/{local_path.name}",
            )
            with open(local_path, "rb") as data:
                blob_client.upload_blob(data, overwrite=True)
            self.logger.info(f"â˜ï¸ Uploaded {local_path.name} â†’ blob:{BLOB_CONTAINER}/outputs/")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Azure upload failed: {e}")


# ============================================================
# Helper: Fetch alternative titles with basic retry logic
# ============================================================
def fetch_alt_titles(tmdb_id: str) -> list[str]:
    """Fetch alternative titles for a TMDb movie (retry-safe)."""
    if not USE_ALT_TITLES or not TMDB_API_KEY:
        return []
    url = f"https://api.themoviedb.org/3/movie/{tmdb_id}/alternative_titles"
    for attempt in range(3):
        try:
            r = requests.get(url, params={"api_key": TMDB_API_KEY}, timeout=10)
            r.raise_for_status()
            titles = [t.get("title", "") for t in r.json().get("titles", []) if t.get("title")]
            return [normalize(t) for t in titles]
        except Exception as e:
            if attempt < 2:
                time.sleep(1.5 * (attempt + 1))
            else:
                return []


# ============================================================
# Step 08: Match TMDb (Instrumented)
# ============================================================
class Step08MatchTMDb(BaseStep, SafeWriterMixin):
    def __init__(self, name="Step 08: Match TMDb (Instrumented)", sample=None):
        super().__init__(name=name)
        self.tmdb_norm = TMDB_DIR / "tmdb_movies_normalized.parquet"
        self.mb_candidates = TMDB_DIR / "tmdb_input_candidates_clean.csv"
        self.output_matches = TMDB_DIR / "tmdb_match_results_enhanced.csv"
        self.output_unmatched = TMDB_DIR / "tmdb_match_unmatched.csv"
        self.output_parquet = TMDB_DIR / "tmdb_match_results_enhanced.parquet"
        self.sample = sample

    def _ensure_normalized(self, df: pd.DataFrame, title_col: str) -> pd.DataFrame:
        needs = FORCE_RENORM or ("normalized_title" not in df.columns) or df["normalized_title"].isna().any()
        if needs:
            self.logger.info("ðŸ§¼ (Re)normalizing titles via utils.normalize_for_matching_extended()")
            df["normalized_title"] = df[title_col].fillna("").map(normalize)
        return df

    def run(self):
        self.setup_logger()
        self.logger.info("ðŸš€ Starting Step 08: Match TMDb (Instrumented, Refactored)")

        # --- Load datasets ---
        self.logger.info("ðŸ“¥ Loading normalized TMDB + MB datasets...")
        tmdb_df = pd.read_parquet(self.tmdb_norm)
        mb_path = TMDB_DIR / "tmdb_input_candidates_clean.csv"

        if not mb_path.exists() or os.path.getsize(mb_path) == 0:
            self.logger.warning(f"ðŸª« MB candidate file missing or empty â†’ {mb_path}")
            return

        mb_df = pd.read_csv(mb_path, dtype=str)

        if self.sample and self.sample < len(tmdb_df):
            tmdb_df = tmdb_df.sample(self.sample, random_state=42)
            self.logger.info(f"ðŸ”¬ Using sample of {self.sample:,} TMDB rows for testing.")

        # --- Data validation ---
        self.logger.info(f"TMDB shape={tmdb_df.shape}, MB shape={mb_df.shape}")
        for df, name in [(tmdb_df, "TMDB"), (mb_df, "MB")]:
            nulls = df[["title", "year"]].isna().sum().to_dict()
            self.logger.info(f"{name} null counts: {nulls}")

        tmdb_df = self._ensure_normalized(tmdb_df, "title")
        mb_df = self._ensure_normalized(mb_df, "title")

        tmdb_df["year"] = pd.to_numeric(tmdb_df["year"], errors="coerce")
        mb_df["year"] = pd.to_numeric(mb_df["year"], errors="coerce")
        mb_df = mb_df.dropna(subset=["normalized_title"])

        tmdb_df = tmdb_df.sort_values(["tmdb_id", "year"]).reset_index(drop=True)
        mb_df = mb_df.sort_values(["release_group_id", "year"]).reset_index(drop=True)

        # --- Exact matches ---
        exact = tmdb_df.merge(
            mb_df[["normalized_title", "year", "title", "release_group_id"]],
            on=["normalized_title", "year"],
            how="inner",
            suffixes=("_tmdb", "_mb"),
        )

        exact_matches = [
            {
                "tmdb_id": r.tmdb_id,
                "tmdb_title": r.title_tmdb,
                "tmdb_year": r.year,
                "matched_title": r.title_mb,
                "release_group_id": r.release_group_id,
                "mb_year": r.year,
                "score": 100,
                "match_mode": "exact",
            }
            for r in exact.itertuples(index=False)
        ]
        self.logger.info(f"âœ… Exact matches: {len(exact_matches):,}")

        remaining = tmdb_df[~tmdb_df["tmdb_id"].isin(exact["tmdb_id"])]
        norm_to_mb = {
            r.normalized_title: (r.title, r.release_group_id, r.year)
            for r in mb_df.itertuples(index=False)
        }

        matches, misses = list(exact_matches), []
        alt_rescue_count, no_candidate_count = 0, 0

        # --- Fuzzy matching ---
        for mv in self.progress_iter(remaining.itertuples(index=False), desc="Fuzzy Matching"):
            q_norm, q_year = mv.normalized_title, mv.year
            pool_df = mb_df[mb_df["year"].between(q_year - YEAR_VARIANCE, q_year + YEAR_VARIANCE)]
            pool = pool_df["normalized_title"].tolist()

            if not pool:
                misses.append(
                    {
                        "tmdb_id": mv.tmdb_id,
                        "tmdb_title": mv.title,
                        "tmdb_year": q_year,
                        "reason": "no_candidates",
                    }
                )
                no_candidate_count += 1
                continue

            best_n = process.extract(
                q_norm,
                pool,
                scorer=lambda a, b, **_: int(0.7 * fuzz.token_set_ratio(a, b) + 0.3 * fuzz.partial_ratio(a, b)),
                limit=TOP_N,
                processor=None,
            )
            best_cand, best_score = (best_n[0][0], best_n[0][1]) if best_n else ("", 0)
            match_mode = "fuzzy"

            if best_score < FUZZ_THRESHOLD and USE_ALT_TITLES:
                for alt in fetch_alt_titles(mv.tmdb_id):
                    alt_n = process.extract(
                        alt,
                        pool,
                        scorer=lambda a, b, **_: int(0.7 * fuzz.token_set_ratio(a, b) + 0.3 * fuzz.partial_ratio(a, b)),
                        limit=TOP_N,
                        processor=None,
                    )
                    if alt_n and alt_n[0][1] > best_score:
                        best_cand, best_score = alt_n[0][0], alt_n[0][1]
                        match_mode = "alt_rescue"
                        alt_rescue_count += 1

            if best_score >= FUZZ_THRESHOLD:
                mb_title, rgid, mb_year = norm_to_mb.get(best_cand, ("", "", None))
                matches.append(
                    {
                        "tmdb_id": mv.tmdb_id,
                        "tmdb_title": mv.title,
                        "tmdb_year": q_year,
                        "matched_title": mb_title,
                        "release_group_id": rgid,
                        "mb_year": mb_year,
                        "score": int(best_score),
                        "match_mode": match_mode,
                    }
                )
            else:
                misses.append(
                    {
                        "tmdb_id": mv.tmdb_id,
                        "tmdb_title": mv.title,
                        "tmdb_year": q_year,
                        "best_candidate": best_cand,
                        "score": int(best_score),
                    }
                )

            if DEBUG_MODE:
                self.logger.info(
                    f"[DEBUG] {mv.title} ({q_year}) â†’ {best_cand} [{best_score}] mode={match_mode}"
                )

        # --- Outputs ---
        self.safe_overwrite(pd.DataFrame(matches), self.output_matches)
        self.safe_overwrite(pd.DataFrame(misses), self.output_unmatched)
        pd.DataFrame(matches).to_parquet(self.output_parquet, index=False)

        score_hist = (
            pd.DataFrame(
                pd.cut(pd.Series([m["score"] for m in matches]), bins=np.arange(0, 105, 5))
                .value_counts()
                .sort_index()
            )
            .reset_index()
            .rename(columns={"index": "score_bin", 0: "count"})
        )
        self.safe_overwrite(score_hist, TMDB_DIR / "tmdb_fuzzy_score_histogram.csv")

        fuzzy_only = len(matches) - len(exact_matches)
        total_tmdb = len(tmdb_df)
        fuzzy_pct = (len(matches) / total_tmdb * 100) if total_tmdb else 0
        avg_score = pd.DataFrame(matches)["score"].mean() if matches else 0
        median_score = pd.DataFrame(matches)["score"].median() if matches else 0

        metrics = {
            "rows_tmdb": total_tmdb,
            "rows_matched_total": len(matches),
            "rows_matched_exact": len(exact_matches),
            "rows_matched_fuzzy_only": fuzzy_only,
            "overall_match_pct": round(fuzzy_pct, 2),
            "avg_match_score": round(avg_score, 2),
            "median_match_score": round(median_score, 2),
            "rows_alt_rescue": alt_rescue_count,
            "rows_no_candidates": no_candidate_count,
            "source_dir": str(TMDB_DIR),
        }

        self.write_metrics("step08_match_tmdb", metrics)
        self.logger.info(f"ðŸ“ˆ Metrics logged: {metrics}")
        self.logger.info("ðŸŽ¬ [DONE] Step 08 (Instrumented, Refactored) completed successfully.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Step 08: Match TMDb with optional sampling")
    parser.add_argument("--sample", type=int, default=None, help="Number of TMDB rows to process (for testing)")
    args = parser.parse_args()

    Step08MatchTMDb(sample=args.sample).run()
