file,line,value,context,kind,suggestion
step_00_acquire_musicbrainz.py,37,1024.0,"chunk_size = max(1024, CHUNK_SIZE)",unknown,
step_00_acquire_musicbrainz.py,121,6.0,if len(parts) < 6:,threshold,FUZZ_THRESHOLD
step_01_audit_raw.py,46,1000000.0,csv.field_size_limit(1_000_000),row limit,ROW_LIMIT
step_01_audit_raw.py,61,30.0,"for row in self.progress_iter(reader, desc=tsv_path.name[:30], unit=""row"", leave=False):",unknown,
step_02_cleanse_tsv.py,43,1000000.0,csv.field_size_limit(1_000_000),row limit,ROW_LIMIT
step_02_cleanse_tsv.py,123,20.0,sample = df.head(20),row limit,ROW_LIMIT
step_02_cleanse_tsv.py,143,3.0,"metrics[""release_year_coverage""] = round(coverage, 3)",unknown,
step_02_cleanse_tsv.py,147,3.0,self.logger.info(df.head(3).to_string()),unknown,
step_03_util_check_tsv_structure.py,36,1000000.0,csv.field_size_limit(1_000_000),row limit,ROW_LIMIT
step_03b_rehydrate_guids.py,45,1000000.0,csv.field_size_limit(1_000_000),row limit,ROW_LIMIT
step_03b_rehydrate_guids.py,73,6.0,"ac = pd.read_csv(ac_path, sep=""\t"", header=None, low_memory=False, usecols=[0, 6])",unknown,
step_03b_rehydrate_guids.py,78,6.0,"ac_map = dict(zip(ac[0].astype(str), ac[6]))",unknown,
step_03b_rehydrate_guids.py,102,100.0,"coverage_rg = release[""release_group_gid""].notna().mean() * 100",unknown,
step_03b_rehydrate_guids.py,103,100.0,"coverage_ac = release[""artist_credit_gid""].notna().mean() * 100",unknown,
step_03b_rehydrate_guids.py,116,10.0,].head(10),unknown,
step_03b_rehydrate_guids.py,122,40.0,"f""   id={row.get('id', '?')} | name={str(row.get('name', ''))[:40]} | """,unknown,
step_03b_rehydrate_guids.py,130,2.0,"""coverage_release_group"": round(coverage_rg, 2),",unknown,
step_03b_rehydrate_guids.py,131,2.0,"""coverage_artist_credit"": round(coverage_ac, 2),",unknown,
step_04_mb_full_join.py,22,1000000.0,csv.field_size_limit(1_000_000),row limit,ROW_LIMIT
step_04_mb_full_join.py,35,2.0,"for idx in [1, 2, 3]:",unknown,
step_04_mb_full_join.py,35,3.0,"for idx in [1, 2, 3]:",unknown,
step_04_mb_full_join.py,76,2.0,st_map = {r[0]: r[1] for r in rgst_rows if len(r) >= 2},threshold,FUZZ_THRESHOLD
step_04_mb_full_join.py,79,2.0,if len(j) >= 2:,threshold,FUZZ_THRESHOLD
step_04_mb_full_join.py,122,5.0,if len(row) < 5:,threshold,FUZZ_THRESHOLD
step_04_mb_full_join.py,126,3.0,artist_credit_id = row[3] if len(row) > 3 else None,threshold,FUZZ_THRESHOLD
step_04_mb_full_join.py,126,3.0,artist_credit_id = row[3] if len(row) > 3 else None,threshold,FUZZ_THRESHOLD
step_04_mb_full_join.py,127,4.0,release_group_id = row[4] if len(row) > 4 else None,threshold,FUZZ_THRESHOLD
step_04_mb_full_join.py,127,4.0,release_group_id = row[4] if len(row) > 4 else None,threshold,FUZZ_THRESHOLD
step_04_mb_full_join.py,132,3.0,if not ac and len(rg) > 3:,threshold,FUZZ_THRESHOLD
step_04_mb_full_join.py,133,3.0,rg_artist_credit_id = rg[3],unknown,
step_04_mb_full_join.py,139,2.0,"rg_name = rg[2] if len(rg) > 2 else """"",threshold,FUZZ_THRESHOLD
step_04_mb_full_join.py,139,2.0,"rg_name = rg[2] if len(rg) > 2 else """"",threshold,FUZZ_THRESHOLD
step_04_mb_full_join.py,146,10.0,if len(soundtrack_samples) < 10 or random.random() < 0.001:,threshold,FUZZ_THRESHOLD
step_04_mb_full_join.py,146,0.001,if len(soundtrack_samples) < 10 or random.random() < 0.001:,threshold,FUZZ_THRESHOLD
step_04_mb_full_join.py,165,100.0,soundtrack_pct = (soundtrack_count / joined_rows * 100) if joined_rows else 0,row limit,ROW_LIMIT
step_04_mb_full_join.py,169,2.0,"""soundtrack_pct"": round(soundtrack_pct, 2),",unknown,
step_04_mb_full_join.py,180,5.0,for row in soundtrack_samples[:5]:,row limit,ROW_LIMIT
step_05_filter_soundtracks_enhanced.py,47,2.0,if len(header) < 2:,threshold,FUZZ_THRESHOLD
step_05_filter_soundtracks_enhanced.py,49,2.0,"soundtrack_ids = {row[0] for row in reader if len(row) >= 2 and row[1] == ""1""}",threshold,FUZZ_THRESHOLD
step_05_filter_soundtracks_enhanced.py,64,13.0,if len(row) < 13:,threshold,FUZZ_THRESHOLD
step_05_filter_soundtracks_enhanced.py,66,4.0,rgid = row[4].strip(),unknown,
step_05_filter_soundtracks_enhanced.py,119,5.0,if not row or len(row) < 5:,threshold,FUZZ_THRESHOLD
step_05_filter_soundtracks_enhanced.py,126,5.0,if cell.isdigit() and len(cell) >= 5:,threshold,FUZZ_THRESHOLD
step_05_filter_soundtracks_enhanced.py,129,4.0,release_group_id = release_group_id or row[4],unknown,
step_05_filter_soundtracks_enhanced.py,159,0.02,"subset_fraction = getattr(self.config, ""SAMPLE_FRACTION"", 0.02)",row limit,ROW_LIMIT
step_05_filter_soundtracks_enhanced.py,161,42.0,"sample = df.sample(frac=subset_fraction, random_state=42)",row limit,ROW_LIMIT
step_05_filter_soundtracks_enhanced.py,165,100.0,"f""ðŸŽ¯ Saved subset ({subset_fraction*100:.1f}% = {len(sample):,} rows) â†’ {sample_path.name}""",row limit,ROW_LIMIT
step_05_filter_soundtracks_enhanced.py,173,100.0,"""match_pct"": round(100 * matched / max(row_count, 1), 2),",unknown,
step_05_filter_soundtracks_enhanced.py,173,2.0,"""match_pct"": round(100 * matched / max(row_count, 1), 2),",unknown,
step_06_fetch_tmdb.py,31,1000.0,self.max_movies = 1000  # overridden by ROW_LIMIT or Golden Mode,row limit,ROW_LIMIT
step_06_fetch_tmdb.py,32,500.0,self.max_pages = 500,unknown,
step_06_fetch_tmdb.py,96,20.0,"movies, page, per_page = [], 1, 20",unknown,
step_06_fetch_tmdb.py,209,3.0,"def _safe_get(self, url: str, params: dict, retries: int = 3, backoff: float = 2.0):",unknown,
step_06_fetch_tmdb.py,209,2.0,"def _safe_get(self, url: str, params: dict, retries: int = 3, backoff: float = 2.0):",unknown,
step_06_fetch_tmdb.py,213,10.0,"r = self.session.get(url, params=params, timeout=10)",timeout,TIMEOUT_SECONDS
step_07_prepare_tmdb_input.py,28,5.0,"if len(title) > 5 and not all(c in ""0123456789abcdef-"" for c in title.lower()):",threshold,FUZZ_THRESHOLD
step_07_prepare_tmdb_input.py,30,2.0,for part in parts[2:4]:,unknown,
step_07_prepare_tmdb_input.py,30,4.0,for part in parts[2:4]:,unknown,
step_07_prepare_tmdb_input.py,31,3.0,"if len(part.strip()) > 3 and not all(c in ""0123456789abcdef-"" for c in part.lower()):",threshold,FUZZ_THRESHOLD
step_07_prepare_tmdb_input.py,66,3.0,if not nt or len(nt) < 3:,threshold,FUZZ_THRESHOLD
step_07_prepare_tmdb_input.py,77,1900.0,if not (1900 <= yr <= 2025):,threshold,FUZZ_THRESHOLD
step_07_prepare_tmdb_input.py,77,2025.0,if not (1900 <= yr <= 2025):,threshold,FUZZ_THRESHOLD
step_07_prepare_tmdb_input.py,113,1900.0,"(df_tmdb[""year""].between(1900, 2025))",unknown,
step_07_prepare_tmdb_input.py,113,2025.0,"(df_tmdb[""year""].between(1900, 2025))",unknown,
step_07_prepare_tmdb_input.py,114,2.0,"& (df_tmdb[""normalized_title""].str.len() > 2)",threshold,FUZZ_THRESHOLD
step_07_prepare_tmdb_input.py,127,100.0,coverage = (len(merged) / len(out_df) * 100) if len(out_df) else 0,unknown,
step_07_prepare_tmdb_input.py,131,100.0,"merged.head(100).to_csv(diag, index=False)",unknown,
step_07_prepare_tmdb_input.py,143,100.0,"(len(merged) / max(len(out_df), 1)) * 100, 2",unknown,
step_07_prepare_tmdb_input.py,143,2.0,"(len(merged) / max(len(out_df), 1)) * 100, 2",unknown,
step_08_match_instrumented.py,66,10.0,"r = requests.get(url, params={""api_key"": TMDB_API_KEY}, timeout=10)",timeout,TIMEOUT_SECONDS
step_08_match_instrumented.py,71,2.0,if attempt < 2:,threshold,FUZZ_THRESHOLD
step_08_match_instrumented.py,72,1.5,time.sleep(1.5 * (attempt + 1)),timing,API_THROTTLE_SECONDS
step_08_match_instrumented.py,113,42.0,"tmdb_df = tmdb_df.sample(self.sample, random_state=42)",row limit,ROW_LIMIT
step_08_match_instrumented.py,148,100.0,"""score"": 100,",threshold,FUZZ_THRESHOLD
step_08_match_instrumented.py,185,0.7,"scorer=lambda a, b, **_: int(0.7 * fuzz.token_set_ratio(a, b) + 0.3 * fuzz.partial_ratio(a, b)),",threshold,FUZZ_THRESHOLD
step_08_match_instrumented.py,185,0.3,"scorer=lambda a, b, **_: int(0.7 * fuzz.token_set_ratio(a, b) + 0.3 * fuzz.partial_ratio(a, b)),",threshold,FUZZ_THRESHOLD
step_08_match_instrumented.py,197,0.7,"scorer=lambda a, b, **_: int(0.7 * fuzz.token_set_ratio(a, b) + 0.3 * fuzz.partial_ratio(a, b)),",threshold,FUZZ_THRESHOLD
step_08_match_instrumented.py,197,0.3,"scorer=lambda a, b, **_: int(0.7 * fuzz.token_set_ratio(a, b) + 0.3 * fuzz.partial_ratio(a, b)),",threshold,FUZZ_THRESHOLD
step_08_match_instrumented.py,243,105.0,"pd.cut(pd.Series([m[""score""] for m in matches]), bins=np.arange(0, 105, 5))",threshold,FUZZ_THRESHOLD
step_08_match_instrumented.py,243,5.0,"pd.cut(pd.Series([m[""score""] for m in matches]), bins=np.arange(0, 105, 5))",threshold,FUZZ_THRESHOLD
step_08_match_instrumented.py,254,100.0,fuzzy_pct = (len(matches) / total_tmdb * 100) if total_tmdb else 0,unknown,
step_08_match_instrumented.py,263,2.0,"""overall_match_pct"": round(fuzzy_pct, 2),",unknown,
step_08_match_instrumented.py,264,2.0,"""avg_match_score"": round(avg_score, 2),",threshold,FUZZ_THRESHOLD
step_08_match_instrumented.py,265,2.0,"""median_match_score"": round(median_score, 2),",threshold,FUZZ_THRESHOLD
step_09_apply_rescues.py,14,90.0,"def __init__(self, name=""Step 09: Apply Manual Rescues (Enhanced & Polished)"", threshold: float = 90.0):",threshold,FUZZ_THRESHOLD
step_09_apply_rescues.py,105,100.0,"""match_score"": 100.0,",threshold,FUZZ_THRESHOLD
step_10_enrich_tmdb.py,27,0.25,self.sleep_time = 0.25  # polite throttle between API calls,timing,API_THROTTLE_SECONDS
step_10_enrich_tmdb.py,42,3.0,"def _safe_get(self, url: str, params: dict, retries: int = 3, backoff: float = 1.5):",unknown,
step_10_enrich_tmdb.py,42,1.5,"def _safe_get(self, url: str, params: dict, retries: int = 3, backoff: float = 1.5):",unknown,
step_10_enrich_tmdb.py,46,10.0,"r = requests.get(url, params=params, timeout=10)",timeout,TIMEOUT_SECONDS
step_10b_coverage_audit.py,17,0.85,"def __init__(self, name=""Step 10B: OST Coverage Audit"", similarity_threshold=0.85):",threshold,FUZZ_THRESHOLD
step_10b_coverage_audit.py,126,100.0,"""coverage_pct"": round(coverage * 100, 2),",unknown,
step_10b_coverage_audit.py,126,2.0,"""coverage_pct"": round(coverage * 100, 2),",unknown,
