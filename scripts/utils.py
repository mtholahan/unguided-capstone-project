"""
utils.py ‚Äî Core utilities for the Discogs‚ÜíTMDB pipeline
--------------------------------------------------------
Provides:
  ‚Ä¢ Title normalization & fuzzy matching helpers
  ‚Ä¢ Progress bar wrapper
  ‚Ä¢ Safe request + retry/backoff + caching
  ‚Ä¢ Threaded batch fetch (with MAX_THREADS)
  ‚Ä¢ Rate limiter for TMDB/Discogs API pacing
  ‚Ä¢ File + logging utilities shared by all pipeline steps
"""

import os
import re
import time
import json
import unicodedata
import logging
import concurrent.futures
import threading
from pathlib import Path
from tqdm import tqdm
import requests
from typing import List, Dict, Any, Iterable, Optional

from config import (
    TMDB_API_KEY,
    API_TIMEOUT,
    RETRY_BACKOFF,
    LOG_LEVEL,
    MAX_THREADS,
    SAVE_RAW_JSON,
    DATA_DIR,
    DISCOGS_CONSUMER_KEY,
    DISCOGS_CONSUMER_SECRET,
    DISCOGS_USER_AGENT,
)

try:
    from config import AZURE_SAS_TOKEN
except ImportError:
    AZURE_SAS_TOKEN = None


# ---------------------------------------------------------------------------
# ü™∂ Logging setup (per-module consistency)
# ---------------------------------------------------------------------------
logger = logging.getLogger("Utils")
logger.setLevel(getattr(logging, LOG_LEVEL.upper(), logging.INFO))

# ---------------------------------------------------------------------------
# üïí Rate Limiter (thread-safe API pacing)
# ---------------------------------------------------------------------------
class RateLimiter:
    def __init__(self, rate_per_sec: float = 3.0):
        try:
            self.min_interval = 1.0 / float(rate_per_sec)
        except (TypeError, ValueError):
            self.min_interval = 1.0 / 3.0
        self.last_call = 0.0
        self.lock = threading.Lock()

    def wait(self):
        import time
        elapsed = time.time() - self.last_call
        if elapsed < self.min_interval:
            time.sleep(self.min_interval - elapsed)
        self.last_call = time.time()

    # üëá Add these two methods
    def __enter__(self):
        self.wait()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        return False


# ---------------------------------------------------------------------------
# üéû Title Normalization & Cleaning
# ---------------------------------------------------------------------------
def normalize_title_for_matching(text: str) -> str:
    """Normalize soundtrack or movie titles for robust fuzzy matching."""
    if not isinstance(text, str):
        return ""

    text = text.lower().strip()
    text = unicodedata.normalize("NFKD", text)
    text = "".join(ch for ch in text if not unicodedata.combining(ch))
    text = re.sub(r"[\(\[\{].*?[\)\]\}]", " ", text)

    noise_patterns = [
        r"original motion picture soundtrack",
        r"original soundtrack",
        r"motion picture soundtrack",
        r"complete motion picture score",
        r"deluxe edition",
        r"expanded edition",
        r"\bost\b",
        r"\bsoundtrack\b",
        r"\bscore\b",
    ]
    for pat in noise_patterns:
        text = re.sub(pat, " ", text, flags=re.IGNORECASE)

    text = re.sub(r"[^a-z0-9\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    tokens = [t for t in text.split() if len(t) > 1]
    return " ".join(tokens)


def normalize_for_matching_extended(text: str) -> str:
    """Extended normalization with roman numeral & franchise cleanup."""
    ROMAN_MAP = {
        " i ": " 1 ", " ii ": " 2 ", " iii ": " 3 ", " iv ": " 4 ", " v ": " 5 ",
        " vi ": " 6 ", " vii ": " 7 ", " viii ": " 8 ", " ix ": " 9 ", " x ": " 10 ",
    }
    ARTICLES = {"the", "a", "an"}
    FRANCHISE_WORDS = {"part", "episode", "chapter", "vol", "volume"}

    base = normalize_title_for_matching(text)
    if not base:
        return ""

    s = f" {base} "
    for k, v in ROMAN_MAP.items():
        s = s.replace(k, v)
    s = s.strip()

    toks = s.split()
    while toks and toks[0] in ARTICLES:
        toks = toks[1:]
    s = " ".join(toks)
    s = re.sub(rf"\b({'|'.join(FRANCHISE_WORDS)})\s+\d+\b", " ", s)
    s = re.sub(r"\s+", " ", s).strip()

    return s


def clean_title(text: str) -> str:
    """Lightweight cleaner for display/logging."""
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r"\(.*?\)|\[.*?\]", "", text)
    text = re.sub(r"[^a-z0-9\s]", "", text)
    return re.sub(r"\s+", " ", text).strip()


def is_mostly_digits(s: str, threshold: float = 0.7) -> bool:
    """Return True if a string is mostly digits (catalog IDs, etc.)."""
    if not s:
        return False
    digits = sum(c.isdigit() for c in s)
    return digits / max(len(s), 1) > threshold


def safe_filename(name: str) -> str:
    """Return a filesystem-safe version of a string (Windows-compatible)."""
    return re.sub(r'[^A-Za-z0-9_\-\.]+', '_', name)



# ---------------------------------------------------------------------------
# üß© Progress Bar Factory
# ---------------------------------------------------------------------------
def make_progress_bar(iterable=None, desc=None, total=None, leave=True, **kwargs):
    """Unified tqdm wrapper for iterable or numeric progress."""
    desc = (desc or "Working")[:40]
    if iterable is not None:
        return tqdm(iterable, desc=desc, leave=leave, **kwargs)
    return tqdm(total=total, desc=desc, leave=leave, **kwargs)


# ---------------------------------------------------------------------------
# üåê Safe Request Helper (integrated)
# ---------------------------------------------------------------------------
def safe_request(
    url: str,
    params: dict = None,
    headers: dict = None,
    retries: int = 3,
    timeout: float = API_TIMEOUT,
    backoff: float = RETRY_BACKOFF,
) -> Optional[Dict[str, Any]]:
    """HTTP GET request with retry and backoff logic."""
    for attempt in range(1, retries + 1):
        try:
            r = requests.get(url, params=params, headers=headers, timeout=timeout)
            r.raise_for_status()
            return r.json()
        except requests.exceptions.Timeout:
            logger.warning(f"‚è≥ Timeout ({timeout}s) on attempt {attempt}/{retries} for {url}")
        except requests.exceptions.RequestException as e:
            logger.warning(f"‚ö†Ô∏è Attempt {attempt}/{retries} failed: {e}")
        if attempt < retries:
            sleep = backoff * attempt
            logger.info(f"Retrying in {sleep:.1f}s...")
            time.sleep(sleep)
    logger.error(f"‚ùå Exhausted retries ({retries}) for {url}")
    return None


# ---------------------------------------------------------------------------
# ‚ö° Parallel Batch Fetch (Threaded)
# ---------------------------------------------------------------------------
def batch_fetch(
    urls: Iterable[str],
    params_list: Optional[List[Dict[str, Any]]] = None,
    headers: Optional[Dict[str, str]] = None,
    desc: str = "Fetching batch",
    max_threads: int = MAX_THREADS,
) -> List[Dict[str, Any]]:
    """
    Perform multiple GET requests concurrently (bounded threads).
    Each element in params_list corresponds to a URL.
    """
    results = []
    urls = list(urls)
    params_list = params_list or [{} for _ in urls]

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = []
        for i, url in enumerate(urls):
            p = params_list[i] if i < len(params_list) else {}
            futures.append(executor.submit(safe_request, url, p, headers))
        for f in make_progress_bar(
            concurrent.futures.as_completed(futures), desc=desc, total=len(futures)
        ):
            result = f.result()
            if result:
                results.append(result)
    return results


# ---------------------------------------------------------------------------
# üíæ File Helpers
# ---------------------------------------------------------------------------
def save_json(data: dict, path: Path):
    """Save a JSON file safely (atomic write)."""
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = path.with_suffix(".tmp")
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    os.replace(tmp, path)
    logger.info(f"üíæ Wrote {path.name} ({len(json.dumps(data)):,} bytes)")


def read_json(path: Path) -> Optional[dict]:
    """Load JSON safely, returning None on error."""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not read {path.name}: {e}")
        return None


# ---------------------------------------------------------------------------
# üóÉÔ∏è  Request Cache (to avoid redundant API hits)
# ---------------------------------------------------------------------------
def get_cache_path(url: str, params: dict, cache_dir: Path = DATA_DIR / "cache") -> Path:
    """Return deterministic cache filename for a given URL+params pair."""
    from hashlib import md5
    cache_dir.mkdir(parents=True, exist_ok=True)
    key = f"{url}?{json.dumps(params, sort_keys=True)}".encode("utf-8")
    return cache_dir / f"{md5(key).hexdigest()}.json"



def cached_request(
    url: str,
    params: dict = None,
    headers: dict = None,
    retries: int = 3,
    timeout: float = API_TIMEOUT,
    backoff: float = RETRY_BACKOFF,
    use_cache: bool = True,
) -> Optional[dict]:
    """Generic request wrapper with smart caching and automatic credential injection."""
    import logging
    params = params or {}
    headers = headers or {}

    # ============================================================
    # üß† Auto-Injection Logic (by domain)
    # ============================================================
    # TMDB
    if "themoviedb.org" in url and "api_key" not in params:
        if TMDB_API_KEY:
            params["api_key"] = TMDB_API_KEY
        else:
            logging.warning("‚ö†Ô∏è Missing TMDB_API_KEY in environment ‚Äî TMDB calls may fail.")

    # Discogs
    if "api.discogs.com" in url:
        if DISCOGS_CONSUMER_KEY and DISCOGS_CONSUMER_SECRET:
            params.setdefault("key", DISCOGS_CONSUMER_KEY)
            params.setdefault("secret", DISCOGS_CONSUMER_SECRET)
        headers.setdefault("User-Agent", DISCOGS_USER_AGENT)
        logging.debug("üîë Injected Discogs credentials automatically.")

    # Azure (for REST endpoints, not SDK)
    if "blob.core.windows.net" in url and AZURE_SAS_TOKEN:
        params.setdefault("sv", AZURE_SAS_TOKEN)
        logging.debug("‚òÅÔ∏è Attached Azure SAS token to request.")

    # ============================================================
    # üß± Caching Logic
    # ============================================================
    cache_path = get_cache_path(url, params)
    if use_cache and cache_path.exists():
        return read_json(cache_path)

    # ============================================================
    # üîÅ Execute Request
    # ============================================================
    result = safe_request(url, params, headers, retries, timeout, backoff)
    if result and use_cache:
        save_json(result, cache_path)
    return result