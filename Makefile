# =========================================================
# ðŸ§± Capstone Project Makefile
# =========================================================
# Purpose: streamline environment management and pipeline runs
# Author: Springboard Data Bootcamp Coach (GPT-5)
# ---------------------------------------------------------

# === Configuration ===
VM_IP ?= 172.190.228.102
VM_USER ?= azureuser
SSH_FLAGS = -o StrictHostKeyChecking=no
PYTHON_VENV = pyspark_venv311

# === Primary Targets ===

.PHONY: help rebuild export sync dryrun run clean

help:
	@echo "Available targets:"
	@echo "  make dryrun    - Run validation checks without executing pipeline"
	@echo "  make sync      - Full Anchor â†’ Execute â†’ Validate workflow"
	@echo "  make run       - Execute pipeline after environment validation"
	@echo "  make log       - Display last 5 entries from sync_log.md"
	@echo "  make clean     - Remove cache, temp, and venv artifacts"
	@echo "  make rebuild   - Rebuild local virtual environment"
	@echo "  make export    - Export requirements to Azure VM"


################################################################################
# ðŸ§© ENVIRONMENT REBUILD & EXPORT TARGETS
#
# These targets rebuild the Python virtual environment and synchronize dependency
# inventories for full reproducibility between local and Azure VM environments.
#
# - requirements_stable.txt â†’ curated list of *direct* project dependencies
#   (auto-generated by pipreqs, based on actual imports in codebase)
#
# - requirements_locked.txt â†’ *full* dependency freeze, including all transitive
#   packages, produced via pip freeze
#
# Both files are refreshed automatically after each `make rebuild` or
# `make export` command, ensuring your dependency manifests always reflect the
# current environment state.
################################################################################

rebuild:
	@git pull --quiet
	@bash rebuild_venv.sh
	@source pyspark_venv311/bin/activate && \
	pip freeze > requirements_locked.txt && \
	pipreqs . --force --savepath requirements_stable.txt && \
	echo "ðŸ“¦ Dependency inventories refreshed (stable & locked)."
	@echo "âœ… Local environment rebuilt."

export:
	@bash rebuild_venv.sh --export
	@source pyspark_venv311/bin/activate && \
	pip freeze > requirements_locked.txt && \
	pipreqs . --force --savepath requirements_stable.txt && \
	echo "ðŸ“¦ Dependency inventories refreshed (stable & locked)."
	@echo "ðŸ“¤ Exported requirements to VM ($(VM_USER)@$(VM_IP))."



# ---------------------------------------------------------
# ðŸ” Capstone Sync Loop (Anchor â†’ Execute â†’ Validate)
# ---------------------------------------------------------

sync:
	@echo "ðŸ” Starting Capstone Anchor â†’ Execute â†’ Validate loop..."
	@git pull --quiet
	@bash rebuild_venv.sh
	@bash rebuild_venv.sh --export
	@ssh $(SSH_FLAGS) $(VM_USER)@$(VM_IP) "cd ~/unguided-capstone-project && git pull --quiet && bash rebuild_venv.sh --force"
	@bash check_env.sh
	@bash run_pipeline_safe.sh
	@echo "âœ… Sync cycle completed successfully â€” environment parity verified."

# ---------------------------------------------------------
# ðŸ§ª Dryrun & Validation
# ---------------------------------------------------------

dryrun:
	@echo "ðŸ§ª Performing dry run validation (no pipeline execution)..."
	@bash check_env.sh
	@echo "âœ… Dry run validation complete."

# ---------------------------------------------------------
# ðŸš€ Pipeline Execution
# ---------------------------------------------------------

run:
	@bash check_env.sh
	@bash run_pipeline_safe.sh

# ---------------------------------------------------------
# ðŸ§¹ Cleanup
# ---------------------------------------------------------

clean:
	@echo "ðŸ§¹ Cleaning environment..."
	@rm -rf __pycache__ .pytest_cache *.log logs/*.log *.pyc
	@find . -type d -name "$(PYTHON_VENV)" -exec rm -rf {} +
	@echo "âœ… Cleanup complete."

log:
	@echo "ðŸ“œ Showing last 5 entries from sync_log.md..."
	@tail -n 5 sync_log.md 2>/dev/null || echo "No sync_log.md found yet."
