# =========================================================
# 🧱 Capstone Project Makefile
# =========================================================
# Purpose: streamline environment management and pipeline runs
# Author: Springboard Data Bootcamp Coach (GPT-5)
# ---------------------------------------------------------

# === Configuration ===
VM_IP ?= 172.190.228.102
VM_USER ?= azureuser
SSH_FLAGS = -o StrictHostKeyChecking=no
PYTHON_VENV = pyspark_venv311

# === Primary Targets ===

.PHONY: help rebuild export sync dryrun run clean

help:
	@echo "Available targets:"
	@echo "  make dryrun    - Run validation checks without executing pipeline"
	@echo "  make sync      - Full Anchor → Execute → Validate workflow"
	@echo "  make run       - Execute pipeline after environment validation"
	@echo "  make log       - Display last 5 entries from sync_log.md"
	@echo "  make clean     - Remove cache, temp, and venv artifacts"
	@echo "  make rebuild   - Rebuild local virtual environment"
	@echo "  make export    - Export requirements to Azure VM"


################################################################################
# 🧩 ENVIRONMENT REBUILD & EXPORT TARGETS
#
# These targets rebuild the Python virtual environment and synchronize dependency
# inventories for full reproducibility between local and Azure VM environments.
#
# - requirements_stable.txt → curated list of *direct* project dependencies
#   (auto-generated by pipreqs, based on actual imports in codebase)
#
# - requirements_locked.txt → *full* dependency freeze, including all transitive
#   packages, produced via pip freeze
#
# Both files are refreshed automatically after each `make rebuild` or
# `make export` command, ensuring your dependency manifests always reflect the
# current environment state.
################################################################################

rebuild:
	@git pull --quiet
	@bash rebuild_venv.sh
	@source pyspark_venv311/bin/activate && \
	pip freeze > requirements_locked.txt && \
	pipreqs . --force --savepath requirements_stable.txt && \
	echo "📦 Dependency inventories refreshed (stable & locked)."
	@echo "✅ Local environment rebuilt."

export:
	@bash rebuild_venv.sh --export
	@source pyspark_venv311/bin/activate && \
	pip freeze > requirements_locked.txt && \
	pipreqs . --force --savepath requirements_stable.txt && \
	echo "📦 Dependency inventories refreshed (stable & locked)."
	@echo "📤 Exported requirements to VM ($(VM_USER)@$(VM_IP))."



# ---------------------------------------------------------
# 🔁 Capstone Sync Loop (Anchor → Execute → Validate)
# ---------------------------------------------------------

sync:
	@echo "🔁 Starting Capstone Anchor → Execute → Validate loop..."
	@git pull --quiet
	@bash rebuild_venv.sh
	@bash rebuild_venv.sh --export
	@ssh $(SSH_FLAGS) $(VM_USER)@$(VM_IP) "cd ~/unguided-capstone-project && git pull --quiet && bash rebuild_venv.sh --force"
	@bash check_env.sh
	@bash run_pipeline_safe.sh
	@echo "✅ Sync cycle completed successfully — environment parity verified."

# ---------------------------------------------------------
# 🧪 Dryrun & Validation
# ---------------------------------------------------------

dryrun:
	@echo "🧪 Performing dry run validation (no pipeline execution)..."
	@bash check_env.sh
	@echo "✅ Dry run validation complete."

# ---------------------------------------------------------
# 🚀 Pipeline Execution
# ---------------------------------------------------------

run:
	@bash check_env.sh
	@bash run_pipeline_safe.sh

# ---------------------------------------------------------
# 🧹 Cleanup
# ---------------------------------------------------------

clean:
	@echo "🧹 Cleaning environment..."
	@rm -rf __pycache__ .pytest_cache *.log logs/*.log *.pyc
	@find . -type d -name "$(PYTHON_VENV)" -exec rm -rf {} +
	@echo "✅ Cleanup complete."

log:
	@echo "📜 Showing last 5 entries from sync_log.md..."
	@tail -n 5 sync_log.md 2>/dev/null || echo "No sync_log.md found yet."
