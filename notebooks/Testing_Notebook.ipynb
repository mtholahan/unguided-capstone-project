{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2177de37-828b-4efd-8b94-ecc2d6ae167c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# ğŸ§ª Step 8 Validation Cell â€” \"Deploy Your Code for Testing\"\n",
    "# ---------------------------------------------------------------\n",
    "# Purpose : Lightweight runtime validation of configuration, Spark,\n",
    "#            and basic pipeline functionality prior to production deploy.\n",
    "# Runtime : Databricks 16.4 LTS (Unity Catalog)\n",
    "# Author  : M. Holahan\n",
    "# Date    : 2025-11-04\n",
    "# ===============================================================\n",
    "\n",
    "# COMMAND ----------\n",
    "# âœ… Environment Bootstrap\n",
    "\n",
    "import sys, os, json, importlib\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "# Mirror Pipeline_Runner import pattern\n",
    "sys.path.append(\"/Workspace/Users/.../scripts\")  # adjust to your workspace path\n",
    "config = importlib.import_module(\"scripts.config\")\n",
    "\n",
    "# Build CONFIG dict dynamically from uppercase vars in config.py\n",
    "CONFIG = {\n",
    "    k: getattr(config, k)\n",
    "    for k in dir(config)\n",
    "    if k.isupper() and not k.startswith(\"__\")\n",
    "}\n",
    "\n",
    "# COMMAND ----------\n",
    "# ğŸ” 1ï¸âƒ£ Config Validation â€” simplified and bulletproof\n",
    "print(\"ğŸ” Checking config file integrity...\")\n",
    "\n",
    "try:\n",
    "    print(\"Detected keys:\", list(CONFIG.keys())[:10], \"...\")\n",
    "    print(f\"âœ… Config imported successfully from: {config.__file__}\")\n",
    "    print(json.dumps(CONFIG, indent=2))\n",
    "except Exception as e:\n",
    "    print(\"âŒ Config validation failed:\", e)\n",
    "    raise\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "# âš™ï¸ 2ï¸âƒ£ Spark Session Validation â€” ultra-simple and resilient\n",
    "\n",
    "print(\"\\nğŸš€ Initializing Spark session...\")\n",
    "\n",
    "try:\n",
    "    app_name = CONFIG.get(\"SPARK_APP_NAME\", \"UnguidedCapstoneTest\")  # fallback name\n",
    "    spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "\n",
    "    print(\"âœ… Spark session active.\")\n",
    "    print(\"Spark version:\", spark.version)\n",
    "    print(\"App name:\", spark.sparkContext.appName)\n",
    "    print(\"Master:\", spark.sparkContext.master)\n",
    "except Exception as e:\n",
    "    print(\"âŒ Spark session test failed:\", e)\n",
    "    raise\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "# ğŸ“Š 3ï¸âƒ£ DataFrame Operation Test\n",
    "\n",
    "print(\"\\nğŸ“Š Running basic Spark DataFrame test...\")\n",
    "\n",
    "try:\n",
    "    df = spark.createDataFrame([Row(id=1, name=\"Test\"), Row(id=2, name=\"Validation\")])\n",
    "    count = df.count()\n",
    "    assert count == 2\n",
    "    print(\"âœ… DataFrame test passed â€” row count:\", count)\n",
    "    df.show()\n",
    "except Exception as e:\n",
    "    print(\"âŒ DataFrame validation failed:\", e)\n",
    "    raise\n",
    "\n",
    "# COMMAND ----------\n",
    "# ğŸ” 4ï¸âƒ£ Environment Variable Check\n",
    "\n",
    "print(\"\\nğŸ” Checking for optional API keys...\")\n",
    "for key in [\"TMDB_API_KEY\", \"DISCOGS_TOKEN\"]:\n",
    "    print(f\"{'âœ…' if os.getenv(key) else 'âš ï¸'} {key} {'found' if os.getenv(key) else 'not found'}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# ğŸ“‚ 5ï¸âƒ£ Directory Existence Check\n",
    "\n",
    "print(\"\\nğŸ§© Checking data directories...\")\n",
    "paths = [\n",
    "    CONFIG[\"DATA_DIR\"],\n",
    "    \"/Workspace/Users/.../data/interim\",\n",
    "    \"/Workspace/Users/.../data/processed\"\n",
    "]\n",
    "for p in paths:\n",
    "    print(f\"{'âœ…' if os.path.exists(p) else 'âš ï¸'} Path check: {p}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# ğŸ¯ 6ï¸âƒ£ Summary Report\n",
    "\n",
    "print(\"\\nğŸ‰ Step 8 Testing Complete!\")\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"âœ… Config module imported and verified\")\n",
    "print(\"âœ… Spark session initialized successfully\")\n",
    "print(\"âœ… DataFrame test executed correctly\")\n",
    "print(\"âœ… Environment and paths checked\")\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"All tests PASSED â€” Ready for Step 9 Deployment ğŸš€\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6331115427625252,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Testing_Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
