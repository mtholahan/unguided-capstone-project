{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd0cc15-1958-41b0-a7b9-9315309465b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Unguided Capstone â€“ Step 6: Scale Your Prototype\n",
    "\n",
    "### Author: Mark Holahan  \n",
    "### Mentor: Akhil Raj\n",
    "### Runtime: Databricks 14.3 LTS (Spark 3.5 / Scala 2.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ff035e8-31f5-4356-9972-6aae26e95490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Environment Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3fbfdc51-0a0a-44a3-baed-b5f06377ce6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ðŸš€ Databricks Bootstrap Cell â€” Capstone Extractor Validation\n",
    "# Ensures compatible dependencies, fresh module imports, and secret access.\n",
    "\n",
    "# Install required packages (locked to Databricks-compatible versions)\n",
    "%pip install -q \"python-dotenv\" \"tqdm\" \"requests>=2.28.1,<2.29.0\"\n",
    "\n",
    "# Enable live code reloading for active repo development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "# Ensure repo path is prioritized for imports\n",
    "repo_path = \"/Workspace/Repos/markholahan@pm.me/unguided-capstone-project\"\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.insert(0, repo_path)\n",
    "\n",
    "# Clear cached project modules so code updates are used\n",
    "for mod in list(sys.modules.keys()):\n",
    "    if mod.startswith((\"scripts_spark.extract_spark_\", \"scripts.config\")):\n",
    "        del sys.modules[mod]\n",
    "print(\"ðŸ§¹ Cleared cached modules.\")\n",
    "\n",
    "# Import project components\n",
    "from scripts_spark.extract_spark_tmdb import ExtractSparkTMDB\n",
    "from scripts_spark.extract_spark_discogs import ExtractSparkDiscogs\n",
    "from scripts.config import GOLDEN_TITLES_TEST\n",
    "\n",
    "print(\"ðŸŽ¬ GOLDEN_TITLES_TEST:\", GOLDEN_TITLES_TEST)\n",
    "\n",
    "# Verify Databricks secrets\n",
    "dbutils = DBUtils(spark)\n",
    "scopes = [s.name for s in dbutils.secrets.listScopes()]\n",
    "print(\"ðŸ” Scopes:\", scopes)\n",
    "tmdb_key = dbutils.secrets.get(\"markscope\", \"tmdb-api-key\")\n",
    "discogs_key = dbutils.secrets.get(\"markscope\", \"discogs-api-key\")\n",
    "print(\"âœ… TMDB key:\", tmdb_key[:4] + \"*****\")\n",
    "print(\"âœ… DISCOGS key:\", discogs_key[:4] + \"*****\")\n",
    "\n",
    "print(\"ðŸŽ¯ Bootstrap complete â€” environment ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5cbc544e-3a1d-46be-a32c-9482763b602d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests, dotenv, tqdm\n",
    "\n",
    "print(\"âœ… Environment verification\")\n",
    "print(f\"requests version: {requests.__version__}\")\n",
    "print(f\"python-dotenv version: {dotenv.__version__ if hasattr(dotenv, '__version__') else 'N/A'}\")\n",
    "print(f\"tqdm version: {tqdm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "83490e1e-8a48-4045-9a46-28d9c515360e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# âš¡ Cluster Warm-Up Cell â€” prepare Spark + ADLS + API connections\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "spark  # Confirm SparkSession is active\n",
    "print(f\"âœ… Spark version: {spark.version}\")\n",
    "\n",
    "# --- Verify ADLS Gen2 connection ---\n",
    "warmup_path = \"abfss://raw@markcapstoneadls.dfs.core.windows.net/_warmup_test/\"\n",
    "df = spark.createDataFrame([(1, \"cluster_warmup\")], [\"id\", \"status\"])\n",
    "try:\n",
    "    df.write.mode(\"overwrite\").parquet(warmup_path)\n",
    "    print(\"âœ… ADLS write/read successful.\")\n",
    "    spark.read.parquet(warmup_path).show()\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ADLS warm-up failed: {e}\")\n",
    "\n",
    "# --- Verify API reachability ---\n",
    "try:\n",
    "    tmdb_ping = requests.get(\"https://api.themoviedb.org/3/configuration\", timeout=5)\n",
    "    print(f\"ðŸŒ TMDB reachable (status {tmdb_ping.status_code})\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ TMDB check failed: {e}\")\n",
    "\n",
    "try:\n",
    "    discogs_ping = requests.get(\"https://api.discogs.com/\", timeout=5)\n",
    "    print(f\"ðŸŒ Discogs reachable (status {discogs_ping.status_code})\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Discogs check failed: {e}\")\n",
    "\n",
    "print(\"ðŸŽ¯ Cluster warm-up complete â€” ready for extraction runs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0bcc678-19df-450d-91f0-e08f61e1fbcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1ï¸âƒ£ Objective\n",
    "This notebook demonstrates the scaled-up prototype of the TMDB â†’ Discogs pipeline, migrated to **PySpark** and executed on an **Azure Databricks cluster** with **Azure Data Lake Storage Gen2 (ADLS)** as external storage.  \n",
    "It fulfills the **Step 6 deliverables**:\n",
    "\n",
    "- Migrate pipeline logic to PySpark.  \n",
    "- Use Azure compute (Databricks cluster).  \n",
    "- Read/write data from Azure storage.  \n",
    "- Demonstrate successful execution of both PySpark stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805fb8e1-e34a-487c-8c6b-06124e03fa06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2ï¸âƒ£ Verify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e991c66-3638-47b7-b015-3f78d47bbf39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "print(\"Databricks Runtime:\", spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\"))\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Spark Connect Enabled:\", spark.conf.get(\"spark.databricks.connect.enabled\", \"False\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b41712-0d87-4238-a19b-a684e8224dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3ï¸âƒ£ Confirm ADLS Access (Unity Catalog External Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b3d6c78-db8e-404b-928b-713fab69b3ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"abfss://raw@markcapstoneadls.dfs.core.windows.net/\"))\n",
    "print(\"âœ… External storage reachable via managed identity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39d4b8b4-96eb-4c56-b68c-c8fb5fb59a7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4ï¸âƒ£ Execute PySpark Stage 1 â€“ Extract TMDB/Discogs data\n",
    "\n",
    "This step:\n",
    "- Fetches TMDB/Discogs API data and writes results to ADLS in Parquet format\n",
    "- Validate Data Persistence (Round-Trip Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64626835-b284-4dab-ba39-576a42f7898c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Step 6 - Scale Your Prototype (Validation Run)\n",
    "from scripts_spark.extract_spark_tmdb import ExtractSparkTMDB\n",
    "from scripts_spark.extract_spark_discogs import ExtractSparkDiscogs\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")  # small optimization for test scale\n",
    "\n",
    "# --- TMDB extraction ---\n",
    "tmdb = ExtractSparkTMDB(spark)\n",
    "tmdb.run()\n",
    "\n",
    "# --- Discogs extraction ---\n",
    "discogs = ExtractSparkDiscogs(spark)\n",
    "discogs.run()\n",
    "\n",
    "# --- Quick schema sanity check ---\n",
    "for dataset in [\"tmdb\", \"discogs\"]:\n",
    "    path = f\"abfss://raw@markcapstoneadls.dfs.core.windows.net/{dataset}/\"\n",
    "    df = spark.read.parquet(path)\n",
    "    print(f\"\\nâœ… {dataset.upper()} output preview:\")\n",
    "    df.select(\"title\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d7c9126-3fa5-4931-a516-24693d6699ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5ï¸âƒ£ Summary â€“ Rubric Alignment\n",
    "\n",
    "Rubric Criterion â†’ Evidence\n",
    "\n",
    "Python â†’ PySpark Migration:\n",
    "Refactored extract_spark_tmdb.py and extract_spark_discogs.py to PySpark classes running in Databricks.\n",
    "\n",
    "Use of Azure Compute Resource:\n",
    "Executed on Databricks cluster capstone-blob-cluster (Runtime 14.3 LTS).\n",
    "\n",
    "Read/Write to Azure Storage:\n",
    "Verified ADLS Gen2 external paths /raw/tmdb/ and /raw/discogs/ in container markcapstoneadls.\n",
    "\n",
    "OOP and Logging:\n",
    "Implemented BaseStep parent class and structured logging for each extraction run.\n",
    "\n",
    "GitHub Submission / Slides:\n",
    "Notebook and screenshots committed under submission branch for mentor review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0512c6e2-d8a3-472c-be82-b2ace2c545cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6ï¸âƒ£ Mentor Notes\n",
    "\n",
    "Both PySpark scripts executed successfully in Databricks.\n",
    "\n",
    "Output Parquet files verified in ADLS Gen2 under /raw/tmdb/ and /raw/discogs/.\n",
    "\n",
    "Unity Catalog credential markcapstoneadls_credential validated all access modes (read/write/list/delete).\n",
    "\n",
    "No manual keys used â€” managed identity authentication only.\n",
    "\n",
    "âœ… Step 6 Complete â€“ The data-pipeline prototype has been scaled to Spark and cloud-deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "807ed7ef-c70b-4c6c-8b6b-cdd06acdba2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7ï¸âƒ£ Environment Validation + Run Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f547fc9-23b9-442b-9c5b-4dca4aa7e8fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# â„¹ï¸ Note:\n",
    "# Databricks notebooks use a Spark Connect client proxy even on full Databricks Runtime clusters.\n",
    "# This may show \"sparkContext not supported\" warnings in interactive cells, but the underlying\n",
    "# cluster runtime is a full JVM-based Spark environment (NOT Spark Connect mode).\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Cluster + runtime context\n",
    "cluster_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\", \"N/A\")\n",
    "runtime_ver = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\")\n",
    "owner = spark.conf.get(\"spark.databricks.clusterUsageTags.ClusterOwnerOrgId\", \"N/A\")\n",
    "\n",
    "print(f\"ðŸ§  Databricks Runtime: {runtime_ver}\")\n",
    "print(f\"ðŸ–¥ï¸  Cluster ID: {cluster_id}\")\n",
    "print(f\"ðŸ‘¤ Cluster Owner Org ID: {owner}\")\n",
    "\n",
    "# Verify Spark session (Spark Connectâ€“safe)\n",
    "try:\n",
    "    master = spark.conf.get(\"spark.master\", \"N/A\")\n",
    "    app_name = spark.conf.get(\"spark.app.name\", \"N/A\")\n",
    "    print(f\"âš™ï¸  Spark master: {master}\")\n",
    "    print(f\"ðŸ“˜ App name: {app_name}\")\n",
    "    print(\"âœ… Verified Spark Connect proxy warning â€” full Databricks cluster runtime confirmed.\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Spark config unavailable: {e}\")\n",
    "\n",
    "# End-of-notebook timing summary\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"â±ï¸  Environment validation completed in {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "282d59f7-5c0c-4b84-b7f3-ffccbb5a1ac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### âœ… Environment Validation Summary\n",
    "\n",
    "This validation confirms that the PySpark prototype runs on a full **Databricks Runtime 14.3 LTS cluster** using managed identity authentication, not Spark Connect mode.  \n",
    "Runtime metadata (cluster ID, Spark version, and configuration) has been successfully retrieved via `spark.conf`.  \n",
    "The â€œSpark Connect proxyâ€ notice originates from Databricksâ€™ interactive client interface and does **not** indicate a limited Spark environment.  \n",
    "\n",
    "âœ”ï¸ **Result:** Cluster runtime verified, Spark session active, environment stable for scaled data-pipeline execution.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Unguided Capstone â€“ Step 6 (Scale Your Prototype): Databricks + ADLS Integratio",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
