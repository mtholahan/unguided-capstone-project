{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f55cdeb7-c250-4722-bf59-ae1ec545a7b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Auto-reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06c44429-3866-4a24-b098-df6f2bd19957",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Config Summary\n",
    "import config\n",
    "config.print_mode_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae424d26-387d-4054-839a-5e11853a5a53",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"status\":261},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762319204767}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#  Pipeline_Runner.py ‚Äî v3.5 (Databricks / Mount-less / Config-Driven)\n",
    "#  ---------------------------------------------------------------\n",
    "#  Purpose : Execute full ETL pipeline (Steps 01‚Äì05)\n",
    "#  Runtime : Databricks 16.4 LTS (Unity Catalog)\n",
    "#  Author  : M. Holahan\n",
    "# ================================================================\n",
    "\n",
    "# COMMAND ----------\n",
    "# ‚úÖ Environment bootstrap\n",
    "!pip install -q adlfs fsspec rapidfuzz\n",
    "\n",
    "import sys\n",
    "import inspect\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "import scripts.config as config\n",
    "from scripts.config import spark, print_mode_summary\n",
    "\n",
    "print_mode_summary()\n",
    "\n",
    "# ================================================================\n",
    "#  Import pipeline steps\n",
    "# ================================================================\n",
    "from scripts.extract_spark_tmdb import Step01ExtractSparkTMDB\n",
    "from scripts.extract_spark_discogs import Step02ExtractSparkDiscogs\n",
    "from scripts.prepare_tmdb_discogs_candidates import Step03PrepareTMDBDiscogsCandidates\n",
    "from scripts.validate_schema_alignment import Step04ValidateSchemaAlignment\n",
    "from scripts.match_and_enrich import Step05MatchAndEnrichDBX\n",
    "\n",
    "# Registry (ordered)\n",
    "PIPELINE_STEPS = {\n",
    "    1: Step01ExtractSparkTMDB,\n",
    "    2: Step02ExtractSparkDiscogs,\n",
    "    3: Step03PrepareTMDBDiscogsCandidates,\n",
    "    4: Step04ValidateSchemaAlignment,\n",
    "    5: Step05MatchAndEnrichDBX,\n",
    "}\n",
    "\n",
    "# ================================================================\n",
    "#  Parameter block\n",
    "# ================================================================\n",
    "ACTIVE_STEPS = [2]   # Adjust for partial runs\n",
    "ROW_LIMIT = None                  # Optional limit for debug mode\n",
    "\n",
    "print(f\"\\nüß© Active Steps : {ACTIVE_STEPS}\")\n",
    "print(f\"üîó Intermediate  : {config.INTERMEDIATE_DIR}\")\n",
    "print(f\"üßæ Metrics Path  : {config.METRICS_DIR}\\n\")\n",
    "\n",
    "# ================================================================\n",
    "#  Execute pipeline with structured logging\n",
    "# ================================================================\n",
    "results = []\n",
    "\n",
    "for step_no in ACTIVE_STEPS:\n",
    "    StepClass = PIPELINE_STEPS[step_no]\n",
    "    #importlib.reload(StepClass.__module__ if hasattr(StepClass, \"__module__\") else StepClass)\n",
    "\n",
    "    # Safely reload the module of the step (if already imported)\n",
    "    module_name = StepClass.__module__\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "    \n",
    "    step_name = StepClass.__name__\n",
    "\n",
    "    print(f\"\\nüöÄ Running Step {step_no}: {step_name}\")\n",
    "    t0 = time.time()\n",
    "    status = \"success\"\n",
    "\n",
    "    try:\n",
    "        # Instantiate step (config-driven)\n",
    "        step = StepClass()\n",
    "\n",
    "        # Pass optional limit parameter if present\n",
    "        sig = inspect.signature(step.run)\n",
    "        kwargs = {\"limit\": ROW_LIMIT} if \"limit\" in sig.parameters else {}\n",
    "        df_out = step.run(**kwargs)\n",
    "\n",
    "    except Exception as e:\n",
    "        status = f\"failed: {type(e).__name__}\"\n",
    "        print(f\"‚ö†Ô∏è Step {step_no} ({step_name}) failed: {e}\")\n",
    "        df_out = None\n",
    "\n",
    "    duration = round(time.time() - t0, 2)\n",
    "    results.append({\n",
    "        \"step\": step_no,\n",
    "        \"name\": step_name,\n",
    "        \"duration_sec\": duration,\n",
    "        \"status\": status\n",
    "    })\n",
    "    print(f\"‚úÖ Step {step_no} completed ‚Üí {status.upper()} in {duration}s\")\n",
    "\n",
    "# ================================================================\n",
    "#  Summary logging\n",
    "# ================================================================\n",
    "summary_df = pd.DataFrame(results)\n",
    "display(summary_df)\n",
    "\n",
    "summary_json = summary_df.to_json(orient=\"records\", indent=2)\n",
    "print(f\"\\nüìä Pipeline Summary:\\n{summary_json}\")\n",
    "\n",
    "# Write to ADLS metrics (mount-less safe)\n",
    "summary_output = f\"{config.METRICS_DIR}/pipeline_summary.json\"\n",
    "\n",
    "try:\n",
    "    import fsspec\n",
    "    fs = fsspec.filesystem(\"abfss\", account_name=config.STORAGE_ACCOUNT, anon=False)\n",
    "    with fs.open(summary_output, \"w\") as f:\n",
    "        f.write(summary_json)\n",
    "    print(f\"üì§ Summary uploaded ‚Üí {summary_output}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not upload summary to ADLS: {e}\")\n",
    "\n",
    "print(\"\\nüèÅ Pipeline execution complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aa0238c8-da58-41d7-8e2e-0f000daf98cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Individual # Step 1\n",
    "from scripts.extract_spark_tmdb import Step01ExtractSparkTMDB\n",
    "Step01ExtractSparkTMDB().run(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "310cc87f-2494-49c0-b15c-0b4309c02d2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Individual # Step 2\n",
    "from scripts.extract_spark_discogs import Step02ExtractSparkDiscogs\n",
    "Step02ExtractSparkDiscogs().run(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1f40d1ba-db5c-42f7-83bd-9f4d99112dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Individual # Step 3\n",
    "import importlib\n",
    "import scripts.prepare_tmdb_discogs_candidates\n",
    "\n",
    "# Reload to ensure latest code version\n",
    "importlib.reload(scripts.prepare_tmdb_discogs_candidates)\n",
    "\n",
    "from scripts.prepare_tmdb_discogs_candidates import Step03PrepareTMDBDiscogsCandidates\n",
    "Step03PrepareTMDBDiscogsCandidates().run(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bbbc74cd-1ca4-4a36-8b70-c5ff7e331faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Individual # Step 4\n",
    "import importlib\n",
    "import scripts.validate_schema_alignment\n",
    "\n",
    "importlib.reload(scripts.validate_schema_alignment)\n",
    "\n",
    "from scripts.validate_schema_alignment import Step04ValidateSchemaAlignment\n",
    "Step04ValidateSchemaAlignment().run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "048368f0-4540-497d-a7b2-c96c9f56eceb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Individual # Step 5\n",
    "import importlib\n",
    "import scripts.match_and_enrich\n",
    "\n",
    "importlib.reload(scripts.match_and_enrich)\n",
    "\n",
    "from scripts.match_and_enrich import Step05MatchAndEnrichDBX\n",
    "Step05MatchAndEnrichDBX().run()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8724313004351644,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Pipeline_Runner",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
