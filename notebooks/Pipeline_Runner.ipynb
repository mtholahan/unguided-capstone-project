{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae424d26-387d-4054-839a-5e11853a5a53",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"name\":300},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762131508843}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#  Databricks notebook: Pipeline Runner\n",
    "#  ---------------------------------------------------------------\n",
    "#  Purpose: Execute the full ETL pipeline (Steps 01‚Äì05)\n",
    "#  Environment: Databricks Runtime 16.4 LTS / markscope secrets\n",
    "#  Author: M. Holahan\n",
    "# ================================================================\n",
    "\n",
    "# COMMAND ----------\n",
    "# ‚úÖ Environment bootstrap\n",
    "!pip install -q adlfs fsspec\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "import inspect\n",
    "import time\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "# Validate secrets and storage\n",
    "storage_account = dbutils.secrets.get(\"markscope\", \"azure-storage-account-name\").strip()\n",
    "storage_key = dbutils.secrets.get(\"markscope\", \"azure-storage-account-key\").strip()\n",
    "base_uri = f\"abfss://raw@{storage_account}.dfs.core.windows.net\"\n",
    "print(f\"Connected to ADLS container: {base_uri}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# üß© Parameter block\n",
    "active_steps = [1, 2, 3, 4]  # Choose which steps to run\n",
    "limit = None                  # Optional row cap for test runs\n",
    "\n",
    "# COMMAND ----------\n",
    "# üì¶ Import pipeline steps\n",
    "from scripts_pandas.prepare_tmdb_discogs_candidates import Step03PrepareTMDBDiscogsCandidates\n",
    "from scripts_pandas.validate_schema_alignment import Step04ValidateSchemaAlignment\n",
    "from scripts_spark.extract_spark_tmdb import Step01ExtractSparkTMDB\n",
    "from scripts_spark.extract_spark_discogs import Step02ExtractSparkDiscogs\n",
    "\n",
    "# Step registry\n",
    "pipeline_steps = {\n",
    "    1: Step01ExtractSparkTMDB,\n",
    "    2: Step02ExtractSparkDiscogs,\n",
    "    3: Step03PrepareTMDBDiscogsCandidates,\n",
    "    4: Step04ValidateSchemaAlignment\n",
    "}\n",
    "\n",
    "# COMMAND ----------\n",
    "# üöÄ Run pipeline (robust & always logs metrics)\n",
    "\n",
    "results = []\n",
    "\n",
    "for step_no in active_steps:\n",
    "    StepClass = pipeline_steps[step_no]\n",
    "    init_params = inspect.signature(StepClass.__init__).parameters\n",
    "\n",
    "    # ‚úÖ Only pass Spark if class supports it\n",
    "    if \"spark\" in init_params:\n",
    "        step = StepClass(spark=spark)\n",
    "    else:\n",
    "        step = StepClass()\n",
    "\n",
    "    print(f\"üöÄ Running Step {step_no}: {StepClass.__name__}\")\n",
    "    t0 = time.time()\n",
    "    status = \"success\"\n",
    "    try:\n",
    "        # Run step (with or without limit parameter)\n",
    "        if \"limit\" in step.run.__code__.co_varnames:\n",
    "            df_out = step.run(limit=limit)\n",
    "        else:\n",
    "            df_out = step.run()\n",
    "\n",
    "    except Exception as e:\n",
    "        status = f\"failed: {type(e).__name__}\"\n",
    "        print(f\"‚ö†Ô∏è Step {step_no} ({StepClass.__name__}) failed: {e}\")\n",
    "\n",
    "    duration = round(time.time() - t0, 2)\n",
    "\n",
    "    # ‚úÖ Always record result, even on failure or None return\n",
    "    results.append({\n",
    "        \"step\": step_no,\n",
    "        \"name\": StepClass.__name__,\n",
    "        \"duration_sec\": duration,\n",
    "        \"status\": status\n",
    "    })\n",
    "\n",
    "    print(f\"‚úÖ Step {step_no} ({StepClass.__name__}) finished with status '{status}' in {duration}s\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# üßæ Summary & export\n",
    "summary_df = pd.DataFrame(results)\n",
    "display(summary_df)\n",
    "\n",
    "summary_path = \"/dbfs/tmp/pipeline_summary.json\"\n",
    "summary_df.to_json(summary_path, orient=\"records\", indent=2)\n",
    "print(f\"üìä Summary written to: {summary_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pipeline_Runner",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
