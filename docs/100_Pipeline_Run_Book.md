## üß≠ Big Picture Reset ‚Äî What You Have Now

| Tool / Script         | What It Actually Does                                        | Where to Run It                                  |
| --------------------- | ------------------------------------------------------------ | ------------------------------------------------ |
| **`scripts/main.py`** | Runs the pipeline locally (your ETL steps).                  | Local WSL shell with `venv` activated.           |
| **`deploy_to_vm.sh`** | Syncs latest code from local ‚Üí Azure VM via `rsync`, validates Spark, Python, and pytest on the VM. | Local WSL shell (it SSHs into VM automatically). |
| **`rebuild_venv.sh`** | Rebuilds your local Python environment from scratch (PySpark, pandas, etc.). | Local WSL shell.                                 |
| **`vm_quickops.sh`**  | (Optional) Performs quick Spark + pytest checks remotely after deployment. | Runs automatically at the end of deploy_to_vm.   |

------

## ‚öôÔ∏è How to Run the Pipeline (Local vs. VM)

### üß© **Local (WSL or Linux)**

From your project root:

```
source ~/pyspark_venv311/bin/activate
python3 scripts/main.py
```

- This uses your fixed environment (`clean_path` etc.).
- Use this for Step 8 pytest validation or data pipeline debugging.

‚úÖ **If Spark prints its version and runs enrichment logic ‚Äî you‚Äôre good.**

------

### ‚òÅÔ∏è **On the VM**

You don‚Äôt need to manually SSH anymore.
 Just use the **deployment script**:

```
bash deploy_to_vm.sh
```

It will:

1. `rsync` all your updated files (excluding venvs, data, etc.)
2. SSH into the VM and:
   - Activate the VM‚Äôs `~/pyspark_venv311`
   - Validate Spark, Python, and pytest environment
3. Optionally trigger `vm_quickops.sh` for auto-tests

You‚Äôll see a summary block like:

```
============================
ENV VALIDATION SUMMARY
============================
Python Env:  ‚úÖ
Spark Setup: ‚úÖ
Pytest Run:  ‚úÖ
============================
== VM VALIDATION COMPLETE ==
```