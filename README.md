# Unguided Capstone â€“ TMDB + Discogs Data Pipeline

**Version 3.1.0  |  Step 9 â€“ Deploy Production Code & Process Dataset  |  Status:** ğŸŸ© Stable  |  Branch: `step9-submission`

**Mentor:** Akhil

------

## ğŸ§­ Context Recap

Building upon Step 8â€™s successful test deployment, this phase represents the **production promotion** of the Medallion architecture. All test-validated components were reconfigured under new production-grade compute and storage environments. The pipeline now processes the complete dataset at scale, leveraging Azure-managed orchestration, logging, and lineage tracking.

------

## ğŸ¯ Project Overview

This release delivers the **production deployment** of the TMDB + Discogs Medallion data pipeline.
 The pipeline executes the **Bronze â†’ Silver â†’ Gold** data flow under **Azure Databricks Runtime 16 LTS**, utilizing **Azure Data Lake Storage Gen2** for persistence and **PySpark 3.5** for distributed compute.

Code validated in Step 8 was promoted to production without modification to business logic, ensuring reproducibility. Execution metrics confirm **1,709 strong matches**, schema alignment across all layers, and verified lineage through JSON logs.

------

## ğŸ“š Data Sources

- **TMDB API v3:** Movie metadata
- **Discogs API:** Artist and record release catalog

Combined, these sources enable multi-domain analytics linking film and music metadata. During production runs, data ingestion handled full API pagination and adaptive rate limiting to prevent throttling.

------

## âš™ï¸ Production Objectives

- Deploy finalized PySpark ETL to Azure Databricks cluster at scale
- Persist outputs to **Azure Data Lake Gold** container in `.parquet` format
- Validate lineage, schema, and runtime metrics through automated JSON audit logs
- Document architecture, runtime, and deployment topology per rubric requirements

------

## ğŸ—ï¸ Production Architecture (Updated)

The architecture remains consistent with Step 7, incorporating optimized cluster sizing and Azure cost controls.)

![ungcap_architecture_step9](assets/ungcap_architecture_step9-1762499398410-9.png)

> [!NOTE]
>
> The production configuration preserves the logical topology defined in Step 7 but introduces modular Bicep definitions, Databricks Runtime 16 LTS, and integration with **Azure Monitor + Log Analytics**. These updates improve observability, maintainability, and cost governance.

### **Key Components**

| Layer          | Azure Service                  | Purpose                          |
| -------------- | ------------------------------ | -------------------------------- |
| **Bronze**     | ADLS Container `raw/`          | Raw TMDB + Discogs ingestion     |
| **Silver**     | ADLS Container `intermediate/` | Cleaned and standardized records |
| **Gold**       | ADLS Container `gold/`         | Matched, enriched outputs        |
| **Compute**    | Databricks Cluster             | PySpark execution at scale       |
| **Monitoring** | Azure Log Analytics            | Step 10 dashboard foundation     |

### Azure Databricks Workspace

![databricks_workspace_overview](assets/databricks_workspace_overview-1762499474363-11.png)

### Azure Resources

![azure_resource_groups](assets/azure_resource_groups-1762499491900-13.png)

### ğŸ“˜ **Azure Resource Organization**

| Resource Group                    | Purpose                       | Key Resources                             |
| --------------------------------- | ----------------------------- | ----------------------------------------- |
| **`rg-unguidedcapstone`**         | Core production workspace     | `ungcap-dbws`, `ungcap-kv`, `ungcap-vnet` |
| **`rg-unguidedcapstone-test`**    | Step 9 validation environment | `ungcapstor01`, `ungcapkv01`              |
| **`rg-unguidedcapstone-managed`** | Databricks-managed compute    | Managed by Azure                          |
| **`NetworkWatcherRG`**            | Monitoring workspace          | Diagnostic use only                       |
| **`capstone-databricks-managed`** | Legacy prototype group        | Archived                                  |

> [!NOTE]
> Production workloads execute entirely in `rg-unguidedcapstone`, using managed identities for secure cross-RG access to storage and Key Vault resources.

------

## ğŸš€ Execution Procedure

1. Attach to production cluster (`capstone-prod-cluster`).
2. Configure parameters as appropriate with `config.py`
3. Execute `Pipeline_Runner.ipynb` to process complete TMDB + Discogs dataset.
4. Validate Gold-layer outputs in `wasbs://gold@<storage>.blob.core.windows.net/`.
5. Confirm lineage and runtime logs in `/data/metrics/`.

### Production Run Highlight Log

![data_pipeline_curated_production_log](assets/data_pipeline_curated_production_log-1762499636981-15.png)

------

## ğŸ“Š Pipeline Execution Metrics

| Metric                      | Value                                       |
| --------------------------- | ------------------------------------------- |
| **Total Processed Records** | 39,718 (10,000 TMDB + 29,718 Discogs)       |
| **Strong Matches**          | 1,709                                       |
| **Duration (min)**          | 26:23                                       |
| **Cluster Type**            | Standard Databricks 16 LTS (2-node)         |
| **Cost Optimization**       | Auto-terminate, spot VMs, ephemeral compute |

### Medallion Lineage Summary

| Step               | Layer  | Records Out | Duration (sec) | Output                  |
| ------------------ | ------ | ----------- | -------------- | ----------------------- |
| Extract TMDB       | Bronze | 10,000      | 288            | raw/tmdb                |
| Extract Discogs    | Bronze | 29,718      | 532            | raw/discogs             |
| Prepare Candidates | Silver | 3,605       | 84             | intermediate/candidates |
| Match & Enrich     | Gold   | 1,709       | <1             | gold/matches            |

> **Total Match Rate:** 47.4 %
>  **Run ID:** `20251107T023645`

------

## ğŸ’° Cost Optimization & Resource Management

Production clusters are ephemeral by design â€” automatically terminated post-run.
Azure cost analysis shows 78% cost reduction through use of **Standard_DS3_v2** node class, short-lived job clusters, and active resource cleanup post-deployment.

------

## ğŸ“‚ Repository Structure (Step 9 â€“ Production Deployment)

```
unguided-capstone-project/
â”œâ”€â”€ README.md
â”œâ”€â”€ _databricks.yml
â”œâ”€â”€ architecture/
â”‚ â””â”€â”€ diagrams/
â”œâ”€â”€ assets/
â”‚ â””â”€â”€ Azure main.bicep Orchestrator What-If Output.png
â”œâ”€â”€ config.json
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ cache/
â”‚ â”œâ”€â”€ intermediate/
â”‚ â”œâ”€â”€ logs/
â”‚ â”œâ”€â”€ metrics/
â”‚ â”œâ”€â”€ mock/
â”‚ â”œâ”€â”€ processed/
â”‚ â”œâ”€â”€ raw/
â”‚ â””â”€â”€ validation/
â”œâ”€â”€ evidence/
â”‚ â””â”€â”€ Azure main.bicep Orchestrator What-If Output.png
â”œâ”€â”€ infrastructure/
â”‚ â”œâ”€â”€ databricks.bicep
â”‚ â”œâ”€â”€ functionapp.bicep
â”‚ â”œâ”€â”€ keyvault.bicep
â”‚ â”œâ”€â”€ main.bicep
â”‚ â”œâ”€â”€ monitoring.bicep
â”‚ â”œâ”€â”€ naming_conventions.md
â”‚ â”œâ”€â”€ storage_account.bicep
â”‚ â”œâ”€â”€ ungcap-step8-test.json
â”‚ â””â”€â”€ vnet.bicep
â”œâ”€â”€ logs/
â”‚ â”œâ”€â”€ cleanup.log
â”‚ â”œâ”€â”€ pipeline.log
â”‚ â””â”€â”€ validation/
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ Data_Inspection_Notebook.ipynb
â”‚ â”œâ”€â”€ Pipeline_Runner_Notebook.ipynb
â”‚ â””â”€â”€ Testing_Notebook.ipynb
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ rebuild_venv.sh
â”œâ”€â”€ requirements_cluster.txt
â”œâ”€â”€ requirements_locked.txt
â”œâ”€â”€ requirements_stable.txt
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ pycache/
â”‚ â”œâ”€â”€ base_step.py
â”‚ â”œâ”€â”€ bootstrap.py
â”‚ â”œâ”€â”€ config.py
â”‚ â”œâ”€â”€ extract_spark_discogs.py
â”‚ â”œâ”€â”€ extract_spark_tmdb.py
â”‚ â”œâ”€â”€ inventory_pipeline_outputs.py
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ match_and_enrich.py
â”‚ â”œâ”€â”€ prepare_tmdb_discogs_candidates.py
â”‚ â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ utils.py
â”‚ â”œâ”€â”€ utils_schema.py
â”‚ â””â”€â”€ validate_schema_alignment.py
â”œâ”€â”€ slides/
â”‚ â””â”€â”€ Step_8_Slide_Deck.pptx
â””â”€â”€ tests/
â”œâ”€â”€ abfss:/
â”œâ”€â”€ conftest.py
â”œâ”€â”€ test_pipeline_config.py
â”œâ”€â”€ test_report.txt
â””â”€â”€ test_spark_session.py
```

------

## ğŸ–¼ï¸ Slide Deck Integration

[View Slide Deck â†’ Step9_Presentation.pptx](slides/Step9_Presentation.pptx)

This presentation summarizes:

- Migration from Step 8 test cluster to production
- Finalized architecture and environment configuration
- Runtime performance highlights and lineage proofs
- Gold-layer schema validation and sample outputs

