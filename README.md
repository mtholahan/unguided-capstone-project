# ðŸŽ¬ Unguided Capstone â€“ Discogs â†’ TMDB ETL Prototype
### Springboard Data Engineering Bootcamp Â· Steps 5â€“6 Milestones  
*(Refactored October 2025 â€” Local Spark Baseline + Stable Environment)*

---

## ðŸ§­ Project Overview

This repository contains the **prototype + scaling ETL pipeline** developed for the unguided capstone.  
The pipeline bridges **music metadata (Discogs)** and **film metadata (TMDb)** to explore:

> **Does soundtrack genre impact a filmâ€™s popularity or rating?**

Step 5 established a functional local ETL;  
Step 6 scales the matching logic to **Apache Spark** using a stable PySparkâ€“Pandas stack verified on Ubuntu WSL 2 and ready for Azure deployment.

---

## ðŸ§© Mid-Stream Pivot: *MusicBrainz â†’ Discogs*

| Issue with MusicBrainz                     | Discogs Advantage                   |
| ------------------------------------------ | ----------------------------------- |
| Sparse or inconsistent soundtrack tagging  | Explicit *genre* and *style* fields |
| Limited linkage between releases â†” artists | Robust JSON API with stable IDs     |
| Weak genre normalization                   | Broad taxonomy useful for analytics |

**Decision:** pivot to Discogs to improve genre coverage, speed, and data quality for downstream correlation.

---

## ðŸ—ï¸ Repository Structure

```
unguided-capstone-project/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # API pulls
â”‚ â”œâ”€â”€ cache/ # cached JSON
â”‚ â”œâ”€â”€ intermediate/ # harmonized candidate pairs
â”‚ â”œâ”€â”€ processed/ # cleaned, matched data
â”‚ â”œâ”€â”€ metrics/ # run-level metrics
â”‚ â””â”€â”€ tmdb_enriched/
â”œâ”€â”€ logs/ # pipeline + Spark logs
â”œâ”€â”€ scripts/ # step_XX_*.py modules
â”œâ”€â”€ docs/ # readme, changelog, notes
â”œâ”€â”€ evidence/ # screenshots, validation
â”œâ”€â”€ slides/ # presentation deck
â”œâ”€â”€ rebuild_venv.sh # reproducible environment script
â”œâ”€â”€ requirements_stable.txt
â””â”€â”€ tmp/ # transient artifacts (git-ignored)
```



---

## âš™ï¸ Pipeline Architecture

```
Discogs â†’ TMDB
â”‚
â”œâ”€â”€ step_01_acquire_discogs.py
â”œâ”€â”€ step_02_fetch_tmdb.py
â”œâ”€â”€ step_03_prepare_tmdb_input.py
â”œâ”€â”€ step_04_match_discogs_tmdb.py
â”œâ”€â”€ step_05_prototype_pipeline.py
â””â”€â”€ step_06_scale_prototype.py â† PySpark scaling + metrics
```



**Supporting modules:**
- `utils.py` â€“ request caching, rate limiting, logging  
- `base_step.py` â€“ step lifecycle base class  
- `config.py` â€“ environment and path management  

---

## ðŸ”‘ Key Design Features

- âœ… OAuth Discogs API access  
- âœ… Modular ETL orchestration (`main.py`)  
- âœ… Thread-safe caching & logging  
- âœ… Spark UDF for hybrid fuzzy matching (RapidFuzz + year logic)  
- âœ… Automatic metrics + plots (JSON + PNG)  
- âœ… Fully reproducible environment via `rebuild_venv.sh`

---

## ðŸ“Š Validation Snapshot (Step 5)

| Metric                     | Result                     |
| -------------------------- | -------------------------- |
| Titles processed           | 200                        |
| Matched pairs (score â‰¥ 85) | **262 / 262 (100 %)**      |
| Avg match score            | 90.0                       |
| Year alignment Î”           | â‰¤ 1 year for 92 %          |
| Runtime                    | â‰ˆ 3 min (local, 8 threads) |

---

## âš™ï¸ Environment Setup (Stable Baseline for Step 6 â†’ Azure)

This project runs on:
- **Python 3.11**
- **PySpark 3.5.2**
- **Pandas 2.0.3**
- **NumPy 1.26.4**
- **Ubuntu WSL 2 or native Linux**

### 1ï¸âƒ£ Rebuild the Environment

```bash
chmod +x rebuild_venv.sh
./rebuild_venv.sh
```

Creates (or reuses) a venv at ~/pyspark_venv311, installs all pinned packages,
and auto-generates requirements_stable.txt.

To activate later:

```bash
source ~/pyspark_venv311/bin/activate
```

#### ðŸ§  Tip:

You can skip reinstalling dependencies (the default behavior) or force a full rebuild if the environment ever becomes unstable:

```bash
./rebuild_venv.sh          # Reuse existing venv; refresh requirements_stable.txt only  
./rebuild_venv.sh --force  # Remove & recreate venv from scratch
```


Use --force whenever:

- You upgrade Python or PySpark versions

- The environment becomes inconsistent

- Youâ€™re migrating to a new workstation or Azure VM


2ï¸âƒ£ Configure VS Code (Optional)

```
Ctrl + Shift + P â†’ Python: Select Interpreter â†’ /home/mark/pyspark_venv311/bin/python
```

3ï¸âƒ£ Run the Spark Step Locally

```bash
cd /mnt/c/Projects/unguided-capstone-project
source ~/pyspark_venv311/bin/activate
python scripts/step_06_scale_prototype.py
```

Outputs:

- data/intermediate/tmdb_discogs_matches_spark.csv

- data/metrics/step06_spark_metrics.json

- data/metrics/step06_spark_score_distribution.png


4ï¸âƒ£ Deploy to Azure (Next Step)

> [!NOTE]
>
> Platform note:
> This project previously used PowerShell setup scripts (setup_env.ps1, set_spark_env.ps1) for Windows-native PySpark.
> As of Step 6+, all execution occurs under Ubuntu (WSL2) using rebuild_venv.sh, which fully replaces those Windows scripts.
> You can safely remove or archive any .ps1 environment scripts.

On your Azure Spark cluster or VM:

```bash
pip install -r requirements_stable.txt
spark-submit scripts/step_06_scale_prototype.py
```

Ensures identical dependencies between local and cloud environments.

- ðŸ’¡ Notes
  â— Avoid pip freeze > requirements.txt â€” it captures dev-tools and may upgrade core libs.
- The pinned versions above are the last known good combo avoiding Pandas _new_Index serialization issues.

- If upgrading Spark â†’ 4.x, revisit the Pandas UDF patch inside step_06_scale_prototype.py.


## ðŸ—ºï¸ Environment Diagram (Conceptual)

```
graph TD
    A[rebuild_venv.sh] --> B[pyspark_venv311]
    B --> C[VS Code Interpreter]
    B --> D[requirements_stable.txt]
    C --> E[Local Spark Job]
    D --> F[Azure Cluster (spark-submit)]
```

## ðŸš€ Next Steps

1. Upload data & outputs to Azure Blob Storage
2. Execute Step 07 (Deploy Spark Job on Azure)

3. Perform Steps 08â€“10: statistical analysis and visualization

4. Finalize capstone submission with evidence artifacts


Â© 2025 Mark â€” Springboard Data Engineering Bootcamp



