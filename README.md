------

------

## **Project:** Unguided Capstone â€“ TMDB + Discogs Data Pipeline  **Version:** 1.7.0 (Step 7: Deployment Architecture)  **Status:** ðŸŸ© Active  **Branch:** `step7-dev`  **Sprint Window:** Oct 17 â€“ Oct 31, 2025  **Mentor:** Akhil (Recoupling post-pause)

# Unguided Capstone Project â€“ TMDB + Discogs Data Engineering Pipeline

*Springboard Data Engineering Bootcamp â€“ Unguided Capstone*

------

## ðŸŽ¯ Project Overview

This unguided capstone project unifies two open data ecosystems â€” **The Movie Database (TMDB)** and **Discogs** â€” to design and deploy a scalable data pipeline capable of ingesting, transforming, and serving structured entertainment metadata for analytics.

The pipeline demonstrates end-to-end data engineering competency across extraction, transformation, orchestration, and cloud deployment, integrating both **PySpark-based ETL** and **Azure-native infrastructure**.

------

## ðŸ§© Problem Statement

Entertainment metadata is highly fragmented across sources. TMDB specializes in film data; Discogs curates music metadata. Analytical scenarios (e.g., soundtrack correlation or cross-domain artist appearances) require unified datasets. This project builds a reproducible, cloud-scalable pipeline to integrate, process, and expose TMDB and Discogs data for such analysis.

------

## âš™ï¸ Technical Objectives

- Design modular extractors for both TMDB and Discogs APIs.
- Implement a PySpark-based transformation pipeline for schema harmonization.
- Develop scalable orchestration patterns using Databricks and Azure components.
- Introduce Infrastructure-as-Code (IaC) to define deployment topology.
- Validate end-to-end reproducibility through testing and version control.

------

## ðŸ—ï¸ Architecture Summary

### Phase 1â€“6: Data Pipeline Development (Completed)

**Core stack:** Python Â· PySpark Â· Databricks Â· Azure Blob Storage Â· Key Vault

| Step | Focus                                 | Deliverable                                              |
| ---- | ------------------------------------- | -------------------------------------------------------- |
| 1    | Define problem, scope, and objectives | Project charter & dataset exploration                    |
| 2    | API Exploration & Ingestion           | Raw extract scripts for TMDB + Discogs APIs              |
| 3    | Data Modeling & Cleaning              | Schema mapping + transformation prototypes               |
| 4    | Pipeline Refinement                   | ETL orchestration inside Databricks notebooks            |
| 5    | Prototyping                           | Functional multi-source ETL pipeline in Databricks       |
| 6    | Scaling Prototype                     | Optimized cluster config, checkpointing, and job control |

**Step 6 Outcome:**

> The unified ETL pipeline operates fully in Databricks using Spark 3.5 / Runtime 14.3 LTS, writing to raw and silver zones in Azure Blob Storage.  Step 6 was approved by mentor (10/21/25) and serves as the canonical code baseline for cloud deployment design.

------

## â˜ï¸ Step 7 â€“ Create the Deployment Architecture *(Current Stage)*

### ðŸŽ¯ Purpose

This step translates the functional ETL pipeline into a **cloud-deployable architecture**, complete with infrastructure definitions and supporting documentation.  It bridges design and execution â€” demonstrating how each data engineering component maps to Azure resources.

### ðŸ§± Active Deliverables

| File                                                       | Purpose                                                      |
| ---------------------------------------------------------- | ------------------------------------------------------------ |
| `/architecture/diagrams/Step7_Architecture_Diagram.drawio` | Finalized architecture diagram (annotated, color-coded)      |
| `/doc/Step7_Architecture_Diagram.png`                      | A high-resolution PNG export of draw.io diagram              |
| `/docs/step07_architecture.md`                             | Narrative describing component roles and design rationale    |
| `/infrastructure/`                                         | Infrastructure-as-Code (ARM JSON templates + naming conventions) |

### ðŸ§° Current Development Environment

- **Host:** Windows 10 Home (local development)
- **Environment:** PowerShell with active Python virtual environment (venv)
- **IDE:** Visual Studio Code (launched from within venv)
- **Version Control:** Git (branch: `step7-dev`)
- **Cloud Context:** Azure Resource Manager (template-level only, no live deployments yet)

*Note:* Earlier pipeline work (Steps 4â€“6) occurred in **Azure Databricks** within the cloud workspace. Step 7 returns to a **local development environment** to construct and validate the infrastructure scaffolding before testing deployments in Step 8.

### ðŸ§© Development Workflow

1. Activate Python venv in PowerShell and launch VS Code

   ```powershell
   .venv\Scripts\Activate; code .
   ```

   

3. Run the IaC script generator:

   ```powershell
   python create_arms.py
   ```

4. Validate creation of `/infrastructure/` templates.

5. Commit and push to `step7-dev` branch:

   ```powershell
   git add infrastructure/
   git commit -m "Step 7: add IaC scaffolding (ARM skeletons)"
   git push origin step7-dev
   ```

### ðŸ§­ Diagram Overview

The Step 7 architecture defines a modular, cloud-scalable layout:

- **Ingestion:** TMDB and Discogs API extractors.
- **Storage:** Azure Blob Storage (raw/silver/gold zones).
- **Processing:** Azure Databricks workspace executing ETL notebooks.
- **Security:** Azure Key Vault for API keys and credentials (via Managed Identity).
- **Orchestration:** Databricks orchestration.
- **Monitoring:** Azure Monitor + Log Analytics.
- **Serving:** Power BI for analytics and visualization.

Each component is represented in both the diagram and corresponding ARM template skeleton.

### ðŸ§¾ Supporting Documents

| File                                   | Description                                                  |
| -------------------------------------- | ------------------------------------------------------------ |
| `docs/step07_architecture.md`          | 3â€“4 sentence summary and rationale of architectural choices. |
| `infrastructure/naming_conventions.md` | Standardized resource naming guide across Azure assets.      |

------

## ðŸ§  Development Modes Recap

| Mode                 | Description                  | Typical Usage                                      |
| -------------------- | ---------------------------- | -------------------------------------------------- |
| **Local (Windows)**  | VS Code in PowerShell venv   | For IaC creation, doc editing, and version control |
| **Local (Ubuntu)**   | VS Code + Python venv in WSL | For Spark job prototyping (alternate dev path)     |
| **Databricks Cloud** | Notebook-based Spark jobs    | For pipeline execution, scaling, and testing       |

Step 7 occurs entirely in **Local (Windows)** mode.  Steps 8â€“11 will reintroduce the **Databricks Cloud** and Azure-native tools for testing and final deployment.

------

## ðŸš€ Next Step â€“ Step 8: Deploy Code for Testing *(Upcoming)*

- Deploy ARM templates from `/infrastructure/` to create a dedicated test resource group.
- Validate Databricks job linkage and Key Vault access policies.
- Execute sample pipeline run against test data.
- Document testing environment in `docs/step08_testing_environment.md`.

------

## ðŸ“˜ Repository Structure

```
project-root/
â”œâ”€â”€ architecture/
â”‚ â”œâ”€â”€ diagrams/
â”‚ â”‚ â”œâ”€â”€ step7_architecture_draft.drawio
â”‚ â”‚ â””â”€â”€ step7_architecture_draft.png
â”‚ â””â”€â”€ notes_architecture_decisions.md
â”‚
â”œâ”€â”€ archive/ # Legacy / retired scripts
â”‚ â”œâ”€â”€ OLD_step_01_acquire_discogs.py
â”‚ â”œâ”€â”€ OLD_step_02_fetch_tmdb.py
â”‚ â””â”€â”€ step_04_legacy_match_discogs_tmdb.py
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ intermediate/ # Local transformation outputs
â”‚ â”œâ”€â”€ metrics/ # Validation metrics & visuals
â”‚ â””â”€â”€ validation/ # Schema comparison CSVs
â”‚
â”œâ”€â”€ docs/
â”‚ â”œâ”€â”€ 01_Unguided_Capstone_Runbook.md
â”‚ â”œâ”€â”€ 02_Mentor Submission & Cleanup Workflow.md
â”‚ â”œâ”€â”€ GPT Anchors Log.md
â”‚ â”œâ”€â”€ README_TODO.md
â”‚ â””â”€â”€ step07_architecture.md
â”‚
â”œâ”€â”€ evidence/
â”‚ â””â”€â”€ Azure main.bicep Orchestrator What-If Output.png
â”‚
â”œâ”€â”€ infrastructure/
â”‚ â”œâ”€â”€ databricks.bicep
â”‚ â”œâ”€â”€ functionapp.bicep
â”‚ â”œâ”€â”€ keyvault.bicep
â”‚ â”œâ”€â”€ main.bicep
â”‚ â”œâ”€â”€ monitoring.bicep
â”‚ â”œâ”€â”€ storage_account.bicep
â”‚ â”œâ”€â”€ vnet.bicep
â”‚ â”œâ”€â”€ naming_conventions.md
â”‚ â””â”€â”€ storage_account.json # legacy ARM stub (pre-Bicep)
â”‚
â”œâ”€â”€ logs/
â”‚ â”œâ”€â”€ cleanup.log
â”‚ â”œâ”€â”€ pipeline.log
â”‚ â””â”€â”€ validation/validation.log
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ ScratchPad.py.ipynb
â”‚ â””â”€â”€ Unguided Capstone â€“ Step 6 Databricks with ADLS Integration.ipynb
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ step_01_acquire_tmdb.py
â”‚ â”œâ”€â”€ step_02_query_discogs.py
â”‚ â”œâ”€â”€ step_03_prepare_tmdb_input.py
â”‚ â”œâ”€â”€ step_04_validate_schema_alignment.py
â”‚ â”œâ”€â”€ step_05_match_and_enrich.py
â”‚ â”œâ”€â”€ step_06_scale_prototype.py
â”‚ â”œâ”€â”€ QA/ # Quality-assurance utilities
â”‚ â””â”€â”€ utils.py, utils_schema.py
â”‚
â”œâ”€â”€ scripts_spark/ # Spark extract prototypes
â”‚ â”œâ”€â”€ extract_spark_tmdb.py
â”‚ â””â”€â”€ extract_spark_discogs.py
â”‚
â”œâ”€â”€ slides/
â”‚ â”œâ”€â”€ Step_6_Slide_Deck_Updated.pptx
â”‚ â””â”€â”€ Unguided Capstone Remaining Slides.md
â”‚
â”œâ”€â”€ config.json
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```



---

## ðŸ“‘ Evidence for Step 7 Validation

To confirm the infrastructure-as-code design is valid yet cost-neutral, all Bicep templates were verified using **Azure CLI â€œwhat-ifâ€ simulations**.  
These previews confirmed Azure recognizes each resource definition, dependency, and parameter without performing live provisioning.

**Validation Summary**

| Template                     | Validation Command                                           | Result      |
| ---------------------------- | ------------------------------------------------------------ | ----------- |
| `storage_account.bicep`      | `az deployment group what-if --template-file infrastructure/storage_account.bicep` | âœ… Passed    |
| `vnet.bicep`                 | `az deployment group what-if --template-file infrastructure/vnet.bicep` | âœ… Passed    |
| `keyvault.bicep`             | `az deployment group what-if --template-file infrastructure/keyvault.bicep` | âœ… Passed    |
| `databricks.bicep`           | `az deployment group what-if --template-file infrastructure/databricks.bicep` | âœ… Passed    |
| `functionapp.bicep`          | `az deployment group what-if --template-file infrastructure/functionapp.bicep` | âœ… Passed    |
| `monitoring.bicep`           | `az deployment group what-if --template-file infrastructure/monitoring.bicep` | âœ… Passed    |
| `main.bicep` (orchestration) | `az deployment group what-if --template-file infrastructure/main.bicep` | âœ… All Green |

> <img src="assets/Azure main.bicep Orchestrator What-If Output.png" alt="Azure main.bicep Orchestrator What-If Output" style="zoom:80%;" />

**Interpretation:**  
All six modules and the main orchestration layer are schema-compliant and ready for controlled deployment in Step 8. No Azure resources were actually provisioned; hence, no costs incurred.

---

## ðŸ§¾ License & Credits

This project is authored by **M. Holahan** as part of the **Springboard Data Engineering Bootcamp** capstone series.  External APIs used include [TMDB](https://developer.themoviedb.org/) and [Discogs](https://www.discogs.com/developers/).

Mentor: Akhil â€” Step 6 approved on 2025-10-21.
 Current sprint: *Paused phase â€“ Step 7 (Architecture & IaC Buildout)* through November 3, 2025.

------

**Status:** ðŸŸ© Active (Step 7 â€“ Deployment Architecture)

**Branch:** `step7-dev`
 **Next Milestone:** Step 7 submission freeze â†’ Step 8 testing deployment setup.

------

### ðŸ“„ Repository Metadata

- **Last Updated:** October 23, 2025
- **Active Branch:** `step7-dev`
- **Next Milestone:** Step 7 submission freeze â†’ Step 8 testing deployment setup
- **Primary Author:** M. Holahan
- **Repository URL:** [GitHub â€“ mtholahan/unguided-capstone-project](https://github.com/mtholahan/unguided-capstone-project)
