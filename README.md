# ðŸŽ¬ Unguided Capstone â€“ Discogs â†’ TMDB ETL Prototype
### Springboard Data Engineering Bootcamp Â· Steps 5â€“6 Milestones  
*(Refactored October 2025 â€” Local Spark Baseline + Stable Environment)*

---

## ðŸ§­ Project Overview

This repository contains the **prototype + scaling ETL pipeline** developed for the unguided capstone.  
The pipeline bridges **music metadata (Discogs)** and **film metadata (TMDb)** to explore:

> **Does soundtrack genre impact a filmâ€™s popularity or rating?**

Step 5 established a functional local ETL;  
Step 6 scales the matching logic to **Apache Spark** using a stable PySparkâ€“Pandas stack verified on Ubuntu WSL 2 and ready for Azure deployment.

---

## ðŸ§© Mid-Stream Pivot: *MusicBrainz â†’ Discogs*

| Issue with MusicBrainz                     | Discogs Advantage                   |
| ------------------------------------------ | ----------------------------------- |
| Sparse or inconsistent soundtrack tagging  | Explicit *genre* and *style* fields |
| Limited linkage between releases â†” artists | Robust JSON API with stable IDs     |
| Weak genre normalization                   | Broad taxonomy useful for analytics |

**Decision:** pivot to Discogs to improve genre coverage, speed, and data quality for downstream correlation.

---

## ðŸ—ï¸ Repository Structure

```
unguided-capstone-project/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # API pulls
â”‚ â”œâ”€â”€ cache/ # cached JSON
â”‚ â”œâ”€â”€ intermediate/ # harmonized candidate pairs
â”‚ â”œâ”€â”€ processed/ # cleaned, matched data
â”‚ â”œâ”€â”€ metrics/ # run-level metrics
â”‚ â””â”€â”€ tmdb_enriched/
â”œâ”€â”€ logs/ # pipeline + Spark logs
â”œâ”€â”€ scripts/ # step_XX_*.py modules
â”œâ”€â”€ docs/ # readme, changelog, notes
â”œâ”€â”€ evidence/ # screenshots, validation
â”œâ”€â”€ slides/ # presentation deck
â”œâ”€â”€ rebuild_venv.sh # reproducible environment script
â”œâ”€â”€ requirements_stable.txt
â””â”€â”€ tmp/ # transient artifacts (git-ignored)
```



---

## âš™ï¸ Pipeline Architecture

```
Discogs â†’ TMDB
â”‚
â”œâ”€â”€ step_01_acquire_discogs.py
â”œâ”€â”€ step_02_fetch_tmdb.py
â”œâ”€â”€ step_03_prepare_tmdb_input.py
â”œâ”€â”€ step_04_match_discogs_tmdb.py
â”œâ”€â”€ step_05_prototype_pipeline.py
â””â”€â”€ step_06_scale_prototype.py â† PySpark scaling + metrics
```



**Supporting modules:**
- `utils.py` â€“ request caching, rate limiting, logging  
- `base_step.py` â€“ step lifecycle base class  
- `config.py` â€“ environment and path management  

---

## ðŸ”‘ Key Design Features

- âœ… OAuth Discogs API access  
- âœ… Modular ETL orchestration (`main.py`)  
- âœ… Thread-safe caching & logging  
- âœ… Spark UDF for hybrid fuzzy matching (RapidFuzz + year logic)  
- âœ… Automatic metrics + plots (JSON + PNG)  
- âœ… Fully reproducible environment via `rebuild_venv.sh`

---

## ðŸ“Š Validation Snapshot (Step 5)

| Metric                     | Result                     |
| -------------------------- | -------------------------- |
| Titles processed           | 200                        |
| Matched pairs (score â‰¥ 85) | **262 / 262 (100 %)**      |
| Avg match score            | 90.0                       |
| Year alignment Î”           | â‰¤ 1 year for 92 %          |
| Runtime                    | â‰ˆ 3 min (local, 8 threads) |

---

## âš™ï¸ Environment Setup (Stable Baseline for Step 6 â†’ Azure)

This project runs on:
- **Python 3.11**
- **PySpark 3.5.2**
- **Pandas 2.0.3**
- **NumPy 1.26.4**
- **Ubuntu WSL 2 or native Linux**

### 1ï¸âƒ£ Rebuild the Environment

```bash
chmod +x rebuild_venv.sh
./rebuild_venv.sh
```

The `rebuild_venv.sh` utility **creates or reuses** your virtual environment at
 `~/pyspark_venv311`, installs all **pinned core packages**, and now auto-generates **two synchronized requirement files** for version management:

- **`requirements_stable.txt`** â†’ your **canonical recipe** of direct dependencies (the â€œhuman-readableâ€ baseline).
   Itâ€™s automatically refreshed whenever the script installs or upgrades core packages.
   Use this file for local rebuilds or when teammates need a consistent yet flexible setup.

- **`requirements_locked.txt`** â†’ your **full dependency snapshot** (the â€œfrozenâ€ state).
   It includes every transitive package and exact version number, ensuring deterministic rebuilds in cloud or CI/CD environments (e.g., Databricks, Azure Pipelines).
   Use this file when you need **byte-for-byte reproducibility**.

  ### âœ… **In summary**

  | Use Case                                        | File                      | Why                                              |
  | ----------------------------------------------- | ------------------------- | ------------------------------------------------ |
  | Local development / exploratory work            | `requirements_stable.txt` | Easier to read and maintain; minimal package set |
  | Production deployment / CI / Databricks cluster | `requirements_locked.txt` | Guarantees exact same environment, every time    |



To activate later:

```bash
source ~/pyspark_venv311/bin/activate
```

#### ðŸ§  Tip:

You can skip reinstalling dependencies (the default behavior) or force a full rebuild if the environment ever becomes unstable:

```bash
./rebuild_venv.sh          # Reuse existing venv; refresh requirements_stable.txt only  
./rebuild_venv.sh --force  # Remove & recreate venv from scratch
```


Use --force whenever:

- You upgrade Python or PySpark versions

- The environment becomes inconsistent

- Youâ€™re migrating to a new workstation or Azure VM


2ï¸âƒ£ Configure VS Code (Optional)

```
Ctrl + Shift + P â†’ Python: Select Interpreter â†’ /home/mark/pyspark_venv311/bin/python
```

3ï¸âƒ£ Run the Spark Step Locally

```bash
cd /mnt/c/Projects/unguided-capstone-project
source ~/pyspark_venv311/bin/activate
python scripts/step_06_scale_prototype.py
```

Outputs:

- data/intermediate/tmdb_discogs_matches_spark.csv

- data/metrics/step06_spark_metrics.json

- data/metrics/step06_spark_score_distribution.png


4ï¸âƒ£ Deploy to Azure (Next Step)

> [!NOTE]
>
> Platform note:
> This project previously used PowerShell setup scripts (setup_env.ps1, set_spark_env.ps1) for Windows-native PySpark.
> As of Step 6+, all execution occurs under Ubuntu (WSL2) using rebuild_venv.sh, which fully replaces those Windows scripts.
> You can safely remove or archive any .ps1 environment scripts.

On your Azure Spark cluster or VM:

```bash
pip install -r requirements_stable.txt
spark-submit scripts/step_06_scale_prototype.py
```

Ensures identical dependencies between local and cloud environments.

- ðŸ’¡ Notes
  â— Avoid pip freeze > requirements.txt â€” it captures dev-tools and may upgrade core libs.
- The pinned versions above are the last known good combo avoiding Pandas _new_Index serialization issues.

- If upgrading Spark â†’ 4.x, revisit the Pandas UDF patch inside step_06_scale_prototype.py.



## ðŸ§¾ Step 6 â€“ Scale Your Prototype

### Purpose
This step scales the prototype ETL pipeline developed in Step 5 to process the full TMDB and Discogs datasets in the cloud using PySpark on Databricks.  
It leverages Sparkâ€™s distributed compute to handle larger volumes efficiently while maintaining the same modular logic and logging framework.

### Prerequisites
- Active **Databricks cluster (Spark 3.5 or higher)**
- Environment variables configured:  
  `TMDB_API_KEY`  and  `DISCOGS_API_KEY`
- Valid Azure Blob access key set in Spark config  
- Repo synced to:  
  `/Workspace/Repos/markholahan@pm.me/unguided-capstone-project`

### Run Instructions
1. Launch your Databricks cluster and open **Pipeline_Preflight.ipynb**  
2. Execute all cells sequentially:  
   - Load environment variables  
   - Initialize Spark session  
   - Run `extract_spark_tmdb.py`  
   - Run `extract_spark_discogs.py`  
   - Verify Parquet writes to Azure Blob  
3. Expected Blob paths:  
   - `/raw/tmdb/`  
   - `/raw/discogs/`  
4. Verify successful notebook completion (green checks).

### Expected Output
- âœ… All cells in Preflight notebook complete successfully  
- âœ… Parquet files persist to Azure Blob Storage  
- âœ… No missing credentials or I/O errors  

### Evidence for Mentor Review
Store screenshots under `/evidence/step6/`:
- `notebook_run_success.png` â€“ Databricks run showing all green checks  
- `blob_file_listing.png` â€“ Azure Blob container with `/raw/tmdb/` and `/raw/discogs/` listings  

---

## ðŸ—ºï¸ Environment Diagram (Conceptual)

```
graph TD
    A[rebuild_venv.sh] --> B[pyspark_venv311]
    B --> C[VS Code Interpreter]
    B --> D[requirements_stable.txt]
    C --> E[Local Spark Job]
    D --> F[Azure Cluster (spark-submit)]
```

## ðŸš€ Next Steps

1. Upload data & outputs to Azure Blob Storage
2. Execute Step 07 (Deploy Spark Job on Azure)

3. Perform Steps 08â€“10: statistical analysis and visualization

4. Finalize capstone submission with evidence artifacts


Â© 2025 Mark â€” Springboard Data Engineering Bootcamp



